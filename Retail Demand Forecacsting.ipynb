{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "loc = pd.read_csv('STORE_LOCATION.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "      <th>PROMO1_FLAG</th>\n",
       "      <th>PROMO2_FLAG</th>\n",
       "      <th>PRICE_REGULAR</th>\n",
       "      <th>PRICE_AFTER_DISC</th>\n",
       "      <th>NUM_CONSULTANT</th>\n",
       "      <th>AUTORIZATION_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40369</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>40370</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>40372</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40373</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>46272</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  product_rk  store_location_rk period_start_dt  demand  \\\n",
       "0           0       40369                309      2016-12-19    29.0   \n",
       "1           1       40370                309      2016-12-19    64.0   \n",
       "2           2       40372                309      2016-12-19    32.0   \n",
       "3           3       40373                309      2016-12-19    10.0   \n",
       "4           4       46272                309      2016-12-19    15.0   \n",
       "\n",
       "   PROMO1_FLAG  PROMO2_FLAG  PRICE_REGULAR  PRICE_AFTER_DISC  NUM_CONSULTANT  \\\n",
       "0          NaN          NaN            NaN               NaN             NaN   \n",
       "1          NaN          NaN            NaN               NaN             NaN   \n",
       "2          NaN          NaN            NaN               NaN             NaN   \n",
       "3          NaN          NaN            NaN               NaN             NaN   \n",
       "4          NaN          NaN            NaN               NaN             NaN   \n",
       "\n",
       "   AUTORIZATION_FLAG  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>908</td>\n",
       "      <td>40369</td>\n",
       "      <td>317</td>\n",
       "      <td>02.12.2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>909</td>\n",
       "      <td>40370</td>\n",
       "      <td>317</td>\n",
       "      <td>02.12.2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910</td>\n",
       "      <td>40372</td>\n",
       "      <td>317</td>\n",
       "      <td>02.12.2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>911</td>\n",
       "      <td>40373</td>\n",
       "      <td>317</td>\n",
       "      <td>02.12.2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912</td>\n",
       "      <td>46272</td>\n",
       "      <td>317</td>\n",
       "      <td>02.12.2019</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  product_rk  store_location_rk period_start_dt  demand\n",
       "0  908       40369                317      02.12.2019     NaN\n",
       "1  909       40370                317      02.12.2019     NaN\n",
       "2  910       40372                317      02.12.2019     NaN\n",
       "3  911       40373                317      02.12.2019     NaN\n",
       "4  912       46272                317      02.12.2019     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE_LOCATION_RK</th>\n",
       "      <th>STORE_LOCATION_LVL_RK4</th>\n",
       "      <th>STORE_LOCATION_LVL_RK3</th>\n",
       "      <th>STORE_LOCATION_LVL_RK2</th>\n",
       "      <th>STORE_LOCATION_LVL_RK1</th>\n",
       "      <th>STORE_OPEN_DTTM</th>\n",
       "      <th>STORE_CLOSURE_DTTM</th>\n",
       "      <th>STORE_LOCATION_ADK_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB1_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB2_hashing</th>\n",
       "      <th>...</th>\n",
       "      <th>STORE_LOCATION_ATTRIB12_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB13_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB14_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB15_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB16_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB17_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB18_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB19_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB20_hashing</th>\n",
       "      <th>STORE_LOCATION_ATTRIB21_hashing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>15Mar2018:14:08:08</td>\n",
       "      <td>01Jan5999:00:00:00</td>\n",
       "      <td>181F921CC957DF5CEC4BF2C18E4A371F</td>\n",
       "      <td>B4E5DABD7AF605C64E1CEF8BE96EDEF0</td>\n",
       "      <td>1B208496FEF79DB5BE9F374BF65865E6</td>\n",
       "      <td>...</td>\n",
       "      <td>6BA1D912365C006B003750C17E810A8C</td>\n",
       "      <td>6BA1D912365C006B003750C17E810A8C</td>\n",
       "      <td>B70EA5F8E1DD1E21F7485AADD433620F</td>\n",
       "      <td>FC5FD090934CE685C1C12E5E0311F1D4</td>\n",
       "      <td>3F0DA22658EF921D0F500BFDC5471DB3</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>15Mar2018:14:08:08</td>\n",
       "      <td>01Jan5999:00:00:00</td>\n",
       "      <td>181F921CC957DF5CEC4BF2C18E4A371F</td>\n",
       "      <td>B4E5DABD7AF605C64E1CEF8BE96EDEF0</td>\n",
       "      <td>1B208496FEF79DB5BE9F374BF65865E6</td>\n",
       "      <td>...</td>\n",
       "      <td>6BA1D912365C006B003750C17E810A8C</td>\n",
       "      <td>6BA1D912365C006B003750C17E810A8C</td>\n",
       "      <td>B70EA5F8E1DD1E21F7485AADD433620F</td>\n",
       "      <td>0A7D98B3F3BA0A5BC89F693FB146FE40</td>\n",
       "      <td>3F0DA22658EF921D0F500BFDC5471DB3</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>15Mar2018:14:08:08</td>\n",
       "      <td>01Jan5999:00:00:00</td>\n",
       "      <td>181F921CC957DF5CEC4BF2C18E4A371F</td>\n",
       "      <td>B4E5DABD7AF605C64E1CEF8BE96EDEF0</td>\n",
       "      <td>1B208496FEF79DB5BE9F374BF65865E6</td>\n",
       "      <td>...</td>\n",
       "      <td>6BA1D912365C006B003750C17E810A8C</td>\n",
       "      <td>6BA1D912365C006B003750C17E810A8C</td>\n",
       "      <td>B70EA5F8E1DD1E21F7485AADD433620F</td>\n",
       "      <td>FC5FD090934CE685C1C12E5E0311F1D4</td>\n",
       "      <td>3F0DA22658EF921D0F500BFDC5471DB3</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>15Mar2018:14:08:08</td>\n",
       "      <td>01Jan5999:00:00:00</td>\n",
       "      <td>181F921CC957DF5CEC4BF2C18E4A371F</td>\n",
       "      <td>B4E5DABD7AF605C64E1CEF8BE96EDEF0</td>\n",
       "      <td>1B208496FEF79DB5BE9F374BF65865E6</td>\n",
       "      <td>...</td>\n",
       "      <td>67B1193B31375E3C959BB5246989D0CA</td>\n",
       "      <td>17AC9AE74D16FBC665FFB4CFC2DF24F5</td>\n",
       "      <td>B70EA5F8E1DD1E21F7485AADD433620F</td>\n",
       "      <td>FC5FD090934CE685C1C12E5E0311F1D4</td>\n",
       "      <td>3F0DA22658EF921D0F500BFDC5471DB3</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>281</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15Mar2018:14:08:08</td>\n",
       "      <td>01Jan5999:00:00:00</td>\n",
       "      <td>181F921CC957DF5CEC4BF2C18E4A371F</td>\n",
       "      <td>B4E5DABD7AF605C64E1CEF8BE96EDEF0</td>\n",
       "      <td>1B208496FEF79DB5BE9F374BF65865E6</td>\n",
       "      <td>...</td>\n",
       "      <td>67B1193B31375E3C959BB5246989D0CA</td>\n",
       "      <td>17AC9AE74D16FBC665FFB4CFC2DF24F5</td>\n",
       "      <td>B70EA5F8E1DD1E21F7485AADD433620F</td>\n",
       "      <td>FC5FD090934CE685C1C12E5E0311F1D4</td>\n",
       "      <td>3F0DA22658EF921D0F500BFDC5471DB3</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "      <td>FF83472786DB3B25D48ABD61502E1D91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STORE_LOCATION_RK  STORE_LOCATION_LVL_RK4  STORE_LOCATION_LVL_RK3  \\\n",
       "0                277                     124                     124   \n",
       "1                278                     124                     124   \n",
       "2                279                     183                     183   \n",
       "3                280                     211                     211   \n",
       "4                281                     221                     221   \n",
       "\n",
       "   STORE_LOCATION_LVL_RK2  STORE_LOCATION_LVL_RK1     STORE_OPEN_DTTM  \\\n",
       "0                      19                       1  15Mar2018:14:08:08   \n",
       "1                      19                       1  15Mar2018:14:08:08   \n",
       "2                      20                       1  15Mar2018:14:08:08   \n",
       "3                      12                       1  15Mar2018:14:08:08   \n",
       "4                       3                       1  15Mar2018:14:08:08   \n",
       "\n",
       "   STORE_CLOSURE_DTTM        STORE_LOCATION_ADK_hashing  \\\n",
       "0  01Jan5999:00:00:00  181F921CC957DF5CEC4BF2C18E4A371F   \n",
       "1  01Jan5999:00:00:00  181F921CC957DF5CEC4BF2C18E4A371F   \n",
       "2  01Jan5999:00:00:00  181F921CC957DF5CEC4BF2C18E4A371F   \n",
       "3  01Jan5999:00:00:00  181F921CC957DF5CEC4BF2C18E4A371F   \n",
       "4  01Jan5999:00:00:00  181F921CC957DF5CEC4BF2C18E4A371F   \n",
       "\n",
       "     STORE_LOCATION_ATTRIB1_hashing    STORE_LOCATION_ATTRIB2_hashing  ...  \\\n",
       "0  B4E5DABD7AF605C64E1CEF8BE96EDEF0  1B208496FEF79DB5BE9F374BF65865E6  ...   \n",
       "1  B4E5DABD7AF605C64E1CEF8BE96EDEF0  1B208496FEF79DB5BE9F374BF65865E6  ...   \n",
       "2  B4E5DABD7AF605C64E1CEF8BE96EDEF0  1B208496FEF79DB5BE9F374BF65865E6  ...   \n",
       "3  B4E5DABD7AF605C64E1CEF8BE96EDEF0  1B208496FEF79DB5BE9F374BF65865E6  ...   \n",
       "4  B4E5DABD7AF605C64E1CEF8BE96EDEF0  1B208496FEF79DB5BE9F374BF65865E6  ...   \n",
       "\n",
       "    STORE_LOCATION_ATTRIB12_hashing   STORE_LOCATION_ATTRIB13_hashing  \\\n",
       "0  6BA1D912365C006B003750C17E810A8C  6BA1D912365C006B003750C17E810A8C   \n",
       "1  6BA1D912365C006B003750C17E810A8C  6BA1D912365C006B003750C17E810A8C   \n",
       "2  6BA1D912365C006B003750C17E810A8C  6BA1D912365C006B003750C17E810A8C   \n",
       "3  67B1193B31375E3C959BB5246989D0CA  17AC9AE74D16FBC665FFB4CFC2DF24F5   \n",
       "4  67B1193B31375E3C959BB5246989D0CA  17AC9AE74D16FBC665FFB4CFC2DF24F5   \n",
       "\n",
       "    STORE_LOCATION_ATTRIB14_hashing   STORE_LOCATION_ATTRIB15_hashing  \\\n",
       "0  B70EA5F8E1DD1E21F7485AADD433620F  FC5FD090934CE685C1C12E5E0311F1D4   \n",
       "1  B70EA5F8E1DD1E21F7485AADD433620F  0A7D98B3F3BA0A5BC89F693FB146FE40   \n",
       "2  B70EA5F8E1DD1E21F7485AADD433620F  FC5FD090934CE685C1C12E5E0311F1D4   \n",
       "3  B70EA5F8E1DD1E21F7485AADD433620F  FC5FD090934CE685C1C12E5E0311F1D4   \n",
       "4  B70EA5F8E1DD1E21F7485AADD433620F  FC5FD090934CE685C1C12E5E0311F1D4   \n",
       "\n",
       "    STORE_LOCATION_ATTRIB16_hashing   STORE_LOCATION_ATTRIB17_hashing  \\\n",
       "0  3F0DA22658EF921D0F500BFDC5471DB3  FF83472786DB3B25D48ABD61502E1D91   \n",
       "1  3F0DA22658EF921D0F500BFDC5471DB3  FF83472786DB3B25D48ABD61502E1D91   \n",
       "2  3F0DA22658EF921D0F500BFDC5471DB3  FF83472786DB3B25D48ABD61502E1D91   \n",
       "3  3F0DA22658EF921D0F500BFDC5471DB3  FF83472786DB3B25D48ABD61502E1D91   \n",
       "4  3F0DA22658EF921D0F500BFDC5471DB3  FF83472786DB3B25D48ABD61502E1D91   \n",
       "\n",
       "    STORE_LOCATION_ATTRIB18_hashing   STORE_LOCATION_ATTRIB19_hashing  \\\n",
       "0  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91   \n",
       "1  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91   \n",
       "2  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91   \n",
       "3  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91   \n",
       "4  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91   \n",
       "\n",
       "    STORE_LOCATION_ATTRIB20_hashing   STORE_LOCATION_ATTRIB21_hashing  \n",
       "0  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91  \n",
       "1  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91  \n",
       "2  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91  \n",
       "3  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91  \n",
       "4  FF83472786DB3B25D48ABD61502E1D91  FF83472786DB3B25D48ABD61502E1D91  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(train.head()), display(test.head()), display(loc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             int64\n",
       "product_rk             int64\n",
       "store_location_rk      int64\n",
       "period_start_dt       object\n",
       "demand               float64\n",
       "PROMO1_FLAG          float64\n",
       "PROMO2_FLAG          float64\n",
       "PRICE_REGULAR        float64\n",
       "PRICE_AFTER_DISC     float64\n",
       "NUM_CONSULTANT       float64\n",
       "AUTORIZATION_FLAG    float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(35344, 11)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0              0\n",
       "product_rk              0\n",
       "store_location_rk       0\n",
       "period_start_dt         0\n",
       "demand               1200\n",
       "PROMO1_FLAG           185\n",
       "PROMO2_FLAG           185\n",
       "PRICE_REGULAR        1127\n",
       "PRICE_AFTER_DISC     1132\n",
       "NUM_CONSULTANT        185\n",
       "AUTORIZATION_FLAG     185\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.dtypes, train.shape, train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2047     1\n",
       "27975    1\n",
       "15693    1\n",
       "13644    1\n",
       "3403     1\n",
       "        ..\n",
       "25305    1\n",
       "31450    1\n",
       "29403    1\n",
       "19164    1\n",
       "0        1\n",
       "Name: Unnamed: 0, Length: 35344, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40373    6074\n",
       "40369    6074\n",
       "40370    6074\n",
       "40372    6073\n",
       "46272    6068\n",
       "96212    4981\n",
       "Name: product_rk, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "798     924\n",
       "1034    924\n",
       "1281    924\n",
       "657     924\n",
       "764     924\n",
       "540     924\n",
       "1202    924\n",
       "517     924\n",
       "1005    924\n",
       "425     924\n",
       "533     924\n",
       "525     923\n",
       "555     923\n",
       "1162    923\n",
       "862     923\n",
       "874     923\n",
       "453     923\n",
       "644     923\n",
       "355     923\n",
       "866     923\n",
       "1191    923\n",
       "1185    923\n",
       "1080    923\n",
       "562     923\n",
       "637     923\n",
       "317     923\n",
       "1173    923\n",
       "380     923\n",
       "1143    922\n",
       "1203    922\n",
       "535     922\n",
       "1079    922\n",
       "557     921\n",
       "504     919\n",
       "1326    859\n",
       "1328    726\n",
       "1316    719\n",
       "1363    669\n",
       "1347    648\n",
       "1380    325\n",
       "309      15\n",
       "Name: store_location_rk, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2019-12-02    240\n",
       "2019-12-09    240\n",
       "2019-10-21    240\n",
       "2019-08-26    240\n",
       "2019-03-11    240\n",
       "             ... \n",
       "2017-01-16    170\n",
       "2017-02-20    170\n",
       "2017-01-30    170\n",
       "2017-02-13    170\n",
       "2017-01-09    170\n",
       "Name: period_start_dt, Length: 159, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.000000     5454\n",
       "3.000000     2278\n",
       "2.000000     2268\n",
       "4.000000     2217\n",
       "5.000000     1981\n",
       "             ... \n",
       "5.688525        1\n",
       "36.546448       1\n",
       "57.453552       1\n",
       "13.377049       1\n",
       "22.133333       1\n",
       "Name: demand, Length: 1806, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0    28323\n",
       "1.0     6414\n",
       "2.0      422\n",
       "Name: PROMO1_FLAG, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0    35159\n",
       "Name: PROMO2_FLAG, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000.000000    5881\n",
       "3000.000000    5879\n",
       "500.000000     5879\n",
       "2000.000000    5879\n",
       "99.000000      1859\n",
       "               ... \n",
       "164.714286        1\n",
       "310.285714        1\n",
       "110.428571        1\n",
       "74.428571         1\n",
       "67.000000         1\n",
       "Name: PRICE_REGULAR, Length: 229, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000.000000    5881\n",
       "3000.000000    5879\n",
       "500.000000     5879\n",
       "2000.000000    5879\n",
       "49.000000      1592\n",
       "               ... \n",
       "68.354286         1\n",
       "48.666667         1\n",
       "191.857143        1\n",
       "67.314490         1\n",
       "192.000000        1\n",
       "Name: PRICE_AFTER_DISC, Length: 1036, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0    35159\n",
       "Name: NUM_CONSULTANT, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0    31913\n",
       "0.0     3246\n",
       "Name: AUTORIZATION_FLAG, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in train.columns:\n",
    "    display(train[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбрасываю ненужные данные, меняю тип данных\n",
    "train.drop(['NUM_CONSULTANT', 'PROMO2_FLAG'], axis=1, inplace=True)\n",
    "train['period_start_dt'] = pd.to_datetime(train['period_start_dt'], format='%Y-%m-%d')\n",
    "train.rename(columns={'Unnamed: 0':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc.drop(loc.columns[2:], axis=1, inplace=True)\n",
    "loc = loc.rename(str.lower, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(loc, how='left', on='store_location_rk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы решить задачу с предсказанием временных рядов при помощи мащинного обучения, нужно создать много разных лагированных фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаю доп. фичи, чтобы уловить сезонные особенности\n",
    "def create_date_features(train):\n",
    "    train['month'] = train.period_start_dt.dt.month\n",
    "#     train['day_of_month'] = train.period_start_dt.dt.day\n",
    "    train['week_of_month'] = train.period_start_dt.dt.day // 7\n",
    "#     train['day_of_year'] = train.period_start_dt.dt.dayofyear\n",
    "    train['week_of_year'] = train.period_start_dt.dt.weekofyear\n",
    "    train['year'] = train.period_start_dt.dt.year\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "      <th>PROMO1_FLAG</th>\n",
       "      <th>PRICE_REGULAR</th>\n",
       "      <th>PRICE_AFTER_DISC</th>\n",
       "      <th>AUTORIZATION_FLAG</th>\n",
       "      <th>store_location_lvl_rk4</th>\n",
       "      <th>month</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40369</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>40370</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>40372</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40373</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>46272</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35339</th>\n",
       "      <td>35537</td>\n",
       "      <td>40370</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35340</th>\n",
       "      <td>35538</td>\n",
       "      <td>40372</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35341</th>\n",
       "      <td>35539</td>\n",
       "      <td>40373</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000.00</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35342</th>\n",
       "      <td>35540</td>\n",
       "      <td>46272</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.29</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35343</th>\n",
       "      <td>35541</td>\n",
       "      <td>96212</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.43</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35344 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  product_rk  store_location_rk period_start_dt  demand  \\\n",
       "0          0       40369                309      2016-12-19    29.0   \n",
       "1          1       40370                309      2016-12-19    64.0   \n",
       "2          2       40372                309      2016-12-19    32.0   \n",
       "3          3       40373                309      2016-12-19    10.0   \n",
       "4          4       46272                309      2016-12-19    15.0   \n",
       "...      ...         ...                ...             ...     ...   \n",
       "35339  35537       40370               1380      2019-12-30     NaN   \n",
       "35340  35538       40372               1380      2019-12-30     NaN   \n",
       "35341  35539       40373               1380      2019-12-30     NaN   \n",
       "35342  35540       46272               1380      2019-12-30     NaN   \n",
       "35343  35541       96212               1380      2019-12-30     NaN   \n",
       "\n",
       "       PROMO1_FLAG  PRICE_REGULAR  PRICE_AFTER_DISC  AUTORIZATION_FLAG  \\\n",
       "0              NaN            NaN               NaN                NaN   \n",
       "1              NaN            NaN               NaN                NaN   \n",
       "2              NaN            NaN               NaN                NaN   \n",
       "3              NaN            NaN               NaN                NaN   \n",
       "4              NaN            NaN               NaN                NaN   \n",
       "...            ...            ...               ...                ...   \n",
       "35339          0.0        1000.00            1000.0                1.0   \n",
       "35340          0.0        2000.00            2000.0                1.0   \n",
       "35341          0.0        3000.00            3000.0                1.0   \n",
       "35342          1.0         284.29             199.0                1.0   \n",
       "35343          1.0         141.43              99.0                1.0   \n",
       "\n",
       "       store_location_lvl_rk4  month  week_of_month  week_of_year  year  \n",
       "0                         203     12              2            51  2016  \n",
       "1                         203     12              2            51  2016  \n",
       "2                         203     12              2            51  2016  \n",
       "3                         203     12              2            51  2016  \n",
       "4                         203     12              2            51  2016  \n",
       "...                       ...    ...            ...           ...   ...  \n",
       "35339                     219     12              4             1  2019  \n",
       "35340                     219     12              4             1  2019  \n",
       "35341                     219     12              4             1  2019  \n",
       "35342                     219     12              4             1  2019  \n",
       "35343                     219     12              4             1  2019  \n",
       "\n",
       "[35344 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_date_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by=['period_start_dt', 'product_rk', 'store_location_rk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лагированные наблюдения\n",
    "def lag_features(dataframe, lags):\n",
    "    for lag in lags:\n",
    "        dataframe['demand_lag_' + str(lag)] = dataframe.groupby([\"product_rk\", \"store_location_rk\"])['demand'].transform(\n",
    "            lambda x: x.shift(lag))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "      <th>PROMO1_FLAG</th>\n",
       "      <th>PRICE_REGULAR</th>\n",
       "      <th>PRICE_AFTER_DISC</th>\n",
       "      <th>AUTORIZATION_FLAG</th>\n",
       "      <th>store_location_lvl_rk4</th>\n",
       "      <th>month</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>year</th>\n",
       "      <th>demand_lag_4</th>\n",
       "      <th>demand_lag_13</th>\n",
       "      <th>demand_lag_26</th>\n",
       "      <th>demand_lag_52</th>\n",
       "      <th>demand_lag_108</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40369</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>40369</td>\n",
       "      <td>317</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>944</td>\n",
       "      <td>40369</td>\n",
       "      <td>355</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>1873</td>\n",
       "      <td>40369</td>\n",
       "      <td>380</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>2802</td>\n",
       "      <td>40369</td>\n",
       "      <td>425</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32975</th>\n",
       "      <td>33150</td>\n",
       "      <td>96212</td>\n",
       "      <td>1326</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>69.300000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.266667</td>\n",
       "      <td>4.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33701</th>\n",
       "      <td>33881</td>\n",
       "      <td>96212</td>\n",
       "      <td>1328</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.021429</td>\n",
       "      <td>84.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>59.466667</td>\n",
       "      <td>18.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34349</th>\n",
       "      <td>34535</td>\n",
       "      <td>96212</td>\n",
       "      <td>1347</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>58.670588</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>35210</td>\n",
       "      <td>96212</td>\n",
       "      <td>1363</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35343</th>\n",
       "      <td>35541</td>\n",
       "      <td>96212</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35344 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  product_rk  store_location_rk period_start_dt  demand  \\\n",
       "0          0       40369                309      2016-12-19    29.0   \n",
       "15        15       40369                317      2016-12-19    50.0   \n",
       "938      944       40369                355      2016-12-19    38.0   \n",
       "1861    1873       40369                380      2016-12-19    39.0   \n",
       "2784    2802       40369                425      2016-12-19   105.0   \n",
       "...      ...         ...                ...             ...     ...   \n",
       "32975  33150       96212               1326      2019-12-30     NaN   \n",
       "33701  33881       96212               1328      2019-12-30     NaN   \n",
       "34349  34535       96212               1347      2019-12-30     NaN   \n",
       "35018  35210       96212               1363      2019-12-30     NaN   \n",
       "35343  35541       96212               1380      2019-12-30     NaN   \n",
       "\n",
       "       PROMO1_FLAG  PRICE_REGULAR  PRICE_AFTER_DISC  AUTORIZATION_FLAG  \\\n",
       "0              NaN            NaN               NaN                NaN   \n",
       "15             NaN            NaN               NaN                NaN   \n",
       "938            NaN            NaN               NaN                NaN   \n",
       "1861           NaN            NaN               NaN                NaN   \n",
       "2784           NaN            NaN               NaN                NaN   \n",
       "...            ...            ...               ...                ...   \n",
       "32975          1.0      99.000000         69.300000                1.0   \n",
       "33701          1.0     121.021429         84.714286                1.0   \n",
       "34349          1.0     141.430000         99.000000                1.0   \n",
       "35018          1.0     141.430000         99.000000                1.0   \n",
       "35343          1.0     141.430000         99.000000                1.0   \n",
       "\n",
       "       store_location_lvl_rk4  month  week_of_month  week_of_year  year  \\\n",
       "0                         203     12              2            51  2016   \n",
       "15                        189     12              2            51  2016   \n",
       "938                        54     12              2            51  2016   \n",
       "1861                      189     12              2            51  2016   \n",
       "2784                      203     12              2            51  2016   \n",
       "...                       ...    ...            ...           ...   ...   \n",
       "32975                     203     12              4             1  2019   \n",
       "33701                      54     12              4             1  2019   \n",
       "34349                     189     12              4             1  2019   \n",
       "35018                      26     12              4             1  2019   \n",
       "35343                     219     12              4             1  2019   \n",
       "\n",
       "       demand_lag_4  demand_lag_13  demand_lag_26  demand_lag_52  \\\n",
       "0               NaN            NaN            NaN            NaN   \n",
       "15              NaN            NaN            NaN            NaN   \n",
       "938             NaN            NaN            NaN            NaN   \n",
       "1861            NaN            NaN            NaN            NaN   \n",
       "2784            NaN            NaN            NaN            NaN   \n",
       "...             ...            ...            ...            ...   \n",
       "32975           NaN            0.0            0.0      21.266667   \n",
       "33701           NaN            0.0           19.0      59.466667   \n",
       "34349           NaN            0.0           20.0      58.670588   \n",
       "35018           NaN            0.0            7.0      32.900000   \n",
       "35343           NaN            0.0            0.0      21.000000   \n",
       "\n",
       "       demand_lag_108  \n",
       "0                 NaN  \n",
       "15                NaN  \n",
       "938               NaN  \n",
       "1861              NaN  \n",
       "2784              NaN  \n",
       "...               ...  \n",
       "32975        4.900000  \n",
       "33701       18.923077  \n",
       "34349             NaN  \n",
       "35018       11.000000  \n",
       "35343             NaN  \n",
       "\n",
       "[35344 rows x 19 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lags = [4, 13, 26, 52, 108]\n",
    "lag_features(train, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лагированное скользящее среднее\n",
    "def roll_mean_features(dataframe, windows):\n",
    "    for window in windows:\n",
    "        dataframe['demand_roll_mean_' + str(window)] = dataframe.groupby([\"product_rk\", \"store_location_rk\"])['demand'].transform(\n",
    "            lambda x: x.shift(5).rolling(window=window).mean())\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "      <th>PROMO1_FLAG</th>\n",
       "      <th>PRICE_REGULAR</th>\n",
       "      <th>PRICE_AFTER_DISC</th>\n",
       "      <th>AUTORIZATION_FLAG</th>\n",
       "      <th>store_location_lvl_rk4</th>\n",
       "      <th>...</th>\n",
       "      <th>demand_lag_4</th>\n",
       "      <th>demand_lag_13</th>\n",
       "      <th>demand_lag_26</th>\n",
       "      <th>demand_lag_52</th>\n",
       "      <th>demand_lag_108</th>\n",
       "      <th>demand_roll_mean_4</th>\n",
       "      <th>demand_roll_mean_13</th>\n",
       "      <th>demand_roll_mean_26</th>\n",
       "      <th>demand_roll_mean_52</th>\n",
       "      <th>demand_roll_mean_108</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40369</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>40369</td>\n",
       "      <td>317</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>944</td>\n",
       "      <td>40369</td>\n",
       "      <td>355</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>1873</td>\n",
       "      <td>40369</td>\n",
       "      <td>380</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>2802</td>\n",
       "      <td>40369</td>\n",
       "      <td>425</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32975</th>\n",
       "      <td>33150</td>\n",
       "      <td>96212</td>\n",
       "      <td>1326</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>69.300000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.266667</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>3.024359</td>\n",
       "      <td>3.372840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33701</th>\n",
       "      <td>33881</td>\n",
       "      <td>96212</td>\n",
       "      <td>1328</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.021429</td>\n",
       "      <td>84.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>59.466667</td>\n",
       "      <td>18.923077</td>\n",
       "      <td>19.525</td>\n",
       "      <td>18.530769</td>\n",
       "      <td>24.150000</td>\n",
       "      <td>25.641667</td>\n",
       "      <td>27.747009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34349</th>\n",
       "      <td>34535</td>\n",
       "      <td>96212</td>\n",
       "      <td>1347</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>58.670588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000</td>\n",
       "      <td>16.615385</td>\n",
       "      <td>18.538462</td>\n",
       "      <td>26.077165</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>35210</td>\n",
       "      <td>96212</td>\n",
       "      <td>1363</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5.615385</td>\n",
       "      <td>8.307692</td>\n",
       "      <td>13.151923</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35343</th>\n",
       "      <td>35541</td>\n",
       "      <td>96212</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35344 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  product_rk  store_location_rk period_start_dt  demand  \\\n",
       "0          0       40369                309      2016-12-19    29.0   \n",
       "15        15       40369                317      2016-12-19    50.0   \n",
       "938      944       40369                355      2016-12-19    38.0   \n",
       "1861    1873       40369                380      2016-12-19    39.0   \n",
       "2784    2802       40369                425      2016-12-19   105.0   \n",
       "...      ...         ...                ...             ...     ...   \n",
       "32975  33150       96212               1326      2019-12-30     NaN   \n",
       "33701  33881       96212               1328      2019-12-30     NaN   \n",
       "34349  34535       96212               1347      2019-12-30     NaN   \n",
       "35018  35210       96212               1363      2019-12-30     NaN   \n",
       "35343  35541       96212               1380      2019-12-30     NaN   \n",
       "\n",
       "       PROMO1_FLAG  PRICE_REGULAR  PRICE_AFTER_DISC  AUTORIZATION_FLAG  \\\n",
       "0              NaN            NaN               NaN                NaN   \n",
       "15             NaN            NaN               NaN                NaN   \n",
       "938            NaN            NaN               NaN                NaN   \n",
       "1861           NaN            NaN               NaN                NaN   \n",
       "2784           NaN            NaN               NaN                NaN   \n",
       "...            ...            ...               ...                ...   \n",
       "32975          1.0      99.000000         69.300000                1.0   \n",
       "33701          1.0     121.021429         84.714286                1.0   \n",
       "34349          1.0     141.430000         99.000000                1.0   \n",
       "35018          1.0     141.430000         99.000000                1.0   \n",
       "35343          1.0     141.430000         99.000000                1.0   \n",
       "\n",
       "       store_location_lvl_rk4  ...  demand_lag_4  demand_lag_13  \\\n",
       "0                         203  ...           NaN            NaN   \n",
       "15                        189  ...           NaN            NaN   \n",
       "938                        54  ...           NaN            NaN   \n",
       "1861                      189  ...           NaN            NaN   \n",
       "2784                      203  ...           NaN            NaN   \n",
       "...                       ...  ...           ...            ...   \n",
       "32975                     203  ...           NaN            0.0   \n",
       "33701                      54  ...           NaN            0.0   \n",
       "34349                     189  ...           NaN            0.0   \n",
       "35018                      26  ...           NaN            0.0   \n",
       "35343                     219  ...           NaN            0.0   \n",
       "\n",
       "       demand_lag_26  demand_lag_52  demand_lag_108  demand_roll_mean_4  \\\n",
       "0                NaN            NaN             NaN                 NaN   \n",
       "15               NaN            NaN             NaN                 NaN   \n",
       "938              NaN            NaN             NaN                 NaN   \n",
       "1861             NaN            NaN             NaN                 NaN   \n",
       "2784             NaN            NaN             NaN                 NaN   \n",
       "...              ...            ...             ...                 ...   \n",
       "32975            0.0      21.266667        4.900000               0.000   \n",
       "33701           19.0      59.466667       18.923077              19.525   \n",
       "34349           20.0      58.670588             NaN               3.000   \n",
       "35018            7.0      32.900000       11.000000               1.250   \n",
       "35343            0.0      21.000000             NaN               2.500   \n",
       "\n",
       "       demand_roll_mean_13  demand_roll_mean_26  demand_roll_mean_52  \\\n",
       "0                      NaN                  NaN                  NaN   \n",
       "15                     NaN                  NaN                  NaN   \n",
       "938                    NaN                  NaN                  NaN   \n",
       "1861                   NaN                  NaN                  NaN   \n",
       "2784                   NaN                  NaN                  NaN   \n",
       "...                    ...                  ...                  ...   \n",
       "32975             0.000000             0.884615             3.024359   \n",
       "33701            18.530769            24.150000            25.641667   \n",
       "34349            16.615385            18.538462            26.077165   \n",
       "35018             5.615385             8.307692            13.151923   \n",
       "35343             4.230769             4.923077                  NaN   \n",
       "\n",
       "       demand_roll_mean_108  \n",
       "0                       NaN  \n",
       "15                      NaN  \n",
       "938                     NaN  \n",
       "1861                    NaN  \n",
       "2784                    NaN  \n",
       "...                     ...  \n",
       "32975              3.372840  \n",
       "33701             27.747009  \n",
       "34349                   NaN  \n",
       "35018                   NaN  \n",
       "35343                   NaN  \n",
       "\n",
       "[35344 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = [4, 13, 26, 52, 108]\n",
    "roll_mean_features(train, windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лагированное скользящее стандартное отклонение\n",
    "def roll_std_features(dataframe, windows):\n",
    "    for window in windows:\n",
    "        dataframe['demand_roll_std_' + str(window)] = dataframe.groupby([\"product_rk\", \"store_location_rk\"])['demand'].transform(\n",
    "            lambda x: x.shift(5).rolling(window=window).std())\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "      <th>PROMO1_FLAG</th>\n",
       "      <th>PRICE_REGULAR</th>\n",
       "      <th>PRICE_AFTER_DISC</th>\n",
       "      <th>AUTORIZATION_FLAG</th>\n",
       "      <th>store_location_lvl_rk4</th>\n",
       "      <th>...</th>\n",
       "      <th>demand_roll_mean_4</th>\n",
       "      <th>demand_roll_mean_13</th>\n",
       "      <th>demand_roll_mean_26</th>\n",
       "      <th>demand_roll_mean_52</th>\n",
       "      <th>demand_roll_mean_108</th>\n",
       "      <th>demand_roll_std_4</th>\n",
       "      <th>demand_roll_std_13</th>\n",
       "      <th>demand_roll_std_26</th>\n",
       "      <th>demand_roll_std_52</th>\n",
       "      <th>demand_roll_std_108</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40369</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>40369</td>\n",
       "      <td>317</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>944</td>\n",
       "      <td>40369</td>\n",
       "      <td>355</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>1873</td>\n",
       "      <td>40369</td>\n",
       "      <td>380</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>2802</td>\n",
       "      <td>40369</td>\n",
       "      <td>425</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32975</th>\n",
       "      <td>33150</td>\n",
       "      <td>96212</td>\n",
       "      <td>1326</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>69.300000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>3.024359</td>\n",
       "      <td>3.372840</td>\n",
       "      <td>9.733398e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.451259</td>\n",
       "      <td>4.262560</td>\n",
       "      <td>3.917583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33701</th>\n",
       "      <td>33881</td>\n",
       "      <td>96212</td>\n",
       "      <td>1328</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.021429</td>\n",
       "      <td>84.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>19.525</td>\n",
       "      <td>18.530769</td>\n",
       "      <td>24.150000</td>\n",
       "      <td>25.641667</td>\n",
       "      <td>27.747009</td>\n",
       "      <td>6.166665e-01</td>\n",
       "      <td>9.305827</td>\n",
       "      <td>10.068681</td>\n",
       "      <td>21.824965</td>\n",
       "      <td>20.580405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34349</th>\n",
       "      <td>34535</td>\n",
       "      <td>96212</td>\n",
       "      <td>1347</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>16.615385</td>\n",
       "      <td>18.538462</td>\n",
       "      <td>26.077165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>19.483063</td>\n",
       "      <td>14.461620</td>\n",
       "      <td>22.242199</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>35210</td>\n",
       "      <td>96212</td>\n",
       "      <td>1363</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5.615385</td>\n",
       "      <td>8.307692</td>\n",
       "      <td>13.151923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.500000e+00</td>\n",
       "      <td>7.263361</td>\n",
       "      <td>6.619784</td>\n",
       "      <td>12.707414</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35343</th>\n",
       "      <td>35541</td>\n",
       "      <td>96212</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.081666e+00</td>\n",
       "      <td>6.734907</td>\n",
       "      <td>5.417919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35344 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  product_rk  store_location_rk period_start_dt  demand  \\\n",
       "0          0       40369                309      2016-12-19    29.0   \n",
       "15        15       40369                317      2016-12-19    50.0   \n",
       "938      944       40369                355      2016-12-19    38.0   \n",
       "1861    1873       40369                380      2016-12-19    39.0   \n",
       "2784    2802       40369                425      2016-12-19   105.0   \n",
       "...      ...         ...                ...             ...     ...   \n",
       "32975  33150       96212               1326      2019-12-30     NaN   \n",
       "33701  33881       96212               1328      2019-12-30     NaN   \n",
       "34349  34535       96212               1347      2019-12-30     NaN   \n",
       "35018  35210       96212               1363      2019-12-30     NaN   \n",
       "35343  35541       96212               1380      2019-12-30     NaN   \n",
       "\n",
       "       PROMO1_FLAG  PRICE_REGULAR  PRICE_AFTER_DISC  AUTORIZATION_FLAG  \\\n",
       "0              NaN            NaN               NaN                NaN   \n",
       "15             NaN            NaN               NaN                NaN   \n",
       "938            NaN            NaN               NaN                NaN   \n",
       "1861           NaN            NaN               NaN                NaN   \n",
       "2784           NaN            NaN               NaN                NaN   \n",
       "...            ...            ...               ...                ...   \n",
       "32975          1.0      99.000000         69.300000                1.0   \n",
       "33701          1.0     121.021429         84.714286                1.0   \n",
       "34349          1.0     141.430000         99.000000                1.0   \n",
       "35018          1.0     141.430000         99.000000                1.0   \n",
       "35343          1.0     141.430000         99.000000                1.0   \n",
       "\n",
       "       store_location_lvl_rk4  ...  demand_roll_mean_4  demand_roll_mean_13  \\\n",
       "0                         203  ...                 NaN                  NaN   \n",
       "15                        189  ...                 NaN                  NaN   \n",
       "938                        54  ...                 NaN                  NaN   \n",
       "1861                      189  ...                 NaN                  NaN   \n",
       "2784                      203  ...                 NaN                  NaN   \n",
       "...                       ...  ...                 ...                  ...   \n",
       "32975                     203  ...               0.000             0.000000   \n",
       "33701                      54  ...              19.525            18.530769   \n",
       "34349                     189  ...               3.000            16.615385   \n",
       "35018                      26  ...               1.250             5.615385   \n",
       "35343                     219  ...               2.500             4.230769   \n",
       "\n",
       "       demand_roll_mean_26  demand_roll_mean_52  demand_roll_mean_108  \\\n",
       "0                      NaN                  NaN                   NaN   \n",
       "15                     NaN                  NaN                   NaN   \n",
       "938                    NaN                  NaN                   NaN   \n",
       "1861                   NaN                  NaN                   NaN   \n",
       "2784                   NaN                  NaN                   NaN   \n",
       "...                    ...                  ...                   ...   \n",
       "32975             0.884615             3.024359              3.372840   \n",
       "33701            24.150000            25.641667             27.747009   \n",
       "34349            18.538462            26.077165                   NaN   \n",
       "35018             8.307692            13.151923                   NaN   \n",
       "35343             4.923077                  NaN                   NaN   \n",
       "\n",
       "       demand_roll_std_4  demand_roll_std_13  demand_roll_std_26  \\\n",
       "0                    NaN                 NaN                 NaN   \n",
       "15                   NaN                 NaN                 NaN   \n",
       "938                  NaN                 NaN                 NaN   \n",
       "1861                 NaN                 NaN                 NaN   \n",
       "2784                 NaN                 NaN                 NaN   \n",
       "...                  ...                 ...                 ...   \n",
       "32975       9.733398e-08            0.000000            1.451259   \n",
       "33701       6.166665e-01            9.305827           10.068681   \n",
       "34349       6.000000e+00           19.483063           14.461620   \n",
       "35018       1.500000e+00            7.263361            6.619784   \n",
       "35343       2.081666e+00            6.734907            5.417919   \n",
       "\n",
       "       demand_roll_std_52  demand_roll_std_108  \n",
       "0                     NaN                  NaN  \n",
       "15                    NaN                  NaN  \n",
       "938                   NaN                  NaN  \n",
       "1861                  NaN                  NaN  \n",
       "2784                  NaN                  NaN  \n",
       "...                   ...                  ...  \n",
       "32975            4.262560             3.917583  \n",
       "33701           21.824965            20.580405  \n",
       "34349           22.242199                  NaN  \n",
       "35018           12.707414                  NaN  \n",
       "35343                 NaN                  NaN  \n",
       "\n",
       "[35344 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = [4, 13, 26, 52, 108]\n",
    "roll_std_features(train, windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лагированное скользящее экспоненциальное среднее\n",
    "def ewm_features(dataframe, alphas, lags):\n",
    "    for alpha in alphas:\n",
    "        for lag in lags:\n",
    "            dataframe['sales_ewm_alpha_' + str(alpha) + \"_lag_\" + str(lag)] = dataframe.groupby([\"product_rk\", \"store_location_rk\"])['demand'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "      <th>PROMO1_FLAG</th>\n",
       "      <th>PRICE_REGULAR</th>\n",
       "      <th>PRICE_AFTER_DISC</th>\n",
       "      <th>AUTORIZATION_FLAG</th>\n",
       "      <th>store_location_lvl_rk4</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_ewm_alpha_0.5_lag_4</th>\n",
       "      <th>sales_ewm_alpha_0.5_lag_13</th>\n",
       "      <th>sales_ewm_alpha_0.5_lag_26</th>\n",
       "      <th>sales_ewm_alpha_0.5_lag_52</th>\n",
       "      <th>sales_ewm_alpha_0.5_lag_108</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_4</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_13</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_26</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_52</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_108</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40369</td>\n",
       "      <td>309</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>40369</td>\n",
       "      <td>317</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>944</td>\n",
       "      <td>40369</td>\n",
       "      <td>355</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>1873</td>\n",
       "      <td>40369</td>\n",
       "      <td>380</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>2802</td>\n",
       "      <td>40369</td>\n",
       "      <td>425</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32975</th>\n",
       "      <td>33150</td>\n",
       "      <td>96212</td>\n",
       "      <td>1326</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>69.300000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.064542</td>\n",
       "      <td>0.730053</td>\n",
       "      <td>10.810018</td>\n",
       "      <td>7.305512</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.165118</td>\n",
       "      <td>1.008419</td>\n",
       "      <td>8.832905</td>\n",
       "      <td>7.717980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33701</th>\n",
       "      <td>33881</td>\n",
       "      <td>96212</td>\n",
       "      <td>1328</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.021429</td>\n",
       "      <td>84.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>19.745785</td>\n",
       "      <td>14.054289</td>\n",
       "      <td>24.738254</td>\n",
       "      <td>69.526814</td>\n",
       "      <td>22.412348</td>\n",
       "      <td>19.587649</td>\n",
       "      <td>16.480274</td>\n",
       "      <td>25.887072</td>\n",
       "      <td>66.681642</td>\n",
       "      <td>22.411728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34349</th>\n",
       "      <td>34535</td>\n",
       "      <td>96212</td>\n",
       "      <td>1347</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.939247</td>\n",
       "      <td>22.447299</td>\n",
       "      <td>16.270752</td>\n",
       "      <td>69.010783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.957552</td>\n",
       "      <td>25.229022</td>\n",
       "      <td>16.198154</td>\n",
       "      <td>68.006972</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>35210</td>\n",
       "      <td>96212</td>\n",
       "      <td>1363</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985987</td>\n",
       "      <td>8.412715</td>\n",
       "      <td>9.960041</td>\n",
       "      <td>37.603911</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.233186</td>\n",
       "      <td>9.665820</td>\n",
       "      <td>10.106544</td>\n",
       "      <td>36.894276</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35343</th>\n",
       "      <td>35541</td>\n",
       "      <td>96212</td>\n",
       "      <td>1380</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>...</td>\n",
       "      <td>2.653056</td>\n",
       "      <td>2.182411</td>\n",
       "      <td>4.307860</td>\n",
       "      <td>34.571429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.542832</td>\n",
       "      <td>3.265311</td>\n",
       "      <td>4.933146</td>\n",
       "      <td>35.020408</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35344 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  product_rk  store_location_rk period_start_dt  demand  \\\n",
       "0          0       40369                309      2016-12-19    29.0   \n",
       "15        15       40369                317      2016-12-19    50.0   \n",
       "938      944       40369                355      2016-12-19    38.0   \n",
       "1861    1873       40369                380      2016-12-19    39.0   \n",
       "2784    2802       40369                425      2016-12-19   105.0   \n",
       "...      ...         ...                ...             ...     ...   \n",
       "32975  33150       96212               1326      2019-12-30     NaN   \n",
       "33701  33881       96212               1328      2019-12-30     NaN   \n",
       "34349  34535       96212               1347      2019-12-30     NaN   \n",
       "35018  35210       96212               1363      2019-12-30     NaN   \n",
       "35343  35541       96212               1380      2019-12-30     NaN   \n",
       "\n",
       "       PROMO1_FLAG  PRICE_REGULAR  PRICE_AFTER_DISC  AUTORIZATION_FLAG  \\\n",
       "0              NaN            NaN               NaN                NaN   \n",
       "15             NaN            NaN               NaN                NaN   \n",
       "938            NaN            NaN               NaN                NaN   \n",
       "1861           NaN            NaN               NaN                NaN   \n",
       "2784           NaN            NaN               NaN                NaN   \n",
       "...            ...            ...               ...                ...   \n",
       "32975          1.0      99.000000         69.300000                1.0   \n",
       "33701          1.0     121.021429         84.714286                1.0   \n",
       "34349          1.0     141.430000         99.000000                1.0   \n",
       "35018          1.0     141.430000         99.000000                1.0   \n",
       "35343          1.0     141.430000         99.000000                1.0   \n",
       "\n",
       "       store_location_lvl_rk4  ...  sales_ewm_alpha_0.5_lag_4  \\\n",
       "0                         203  ...                        NaN   \n",
       "15                        189  ...                        NaN   \n",
       "938                        54  ...                        NaN   \n",
       "1861                      189  ...                        NaN   \n",
       "2784                      203  ...                        NaN   \n",
       "...                       ...  ...                        ...   \n",
       "32975                     203  ...                   0.000252   \n",
       "33701                      54  ...                  19.745785   \n",
       "34349                     189  ...                   1.939247   \n",
       "35018                      26  ...                   0.985987   \n",
       "35343                     219  ...                   2.653056   \n",
       "\n",
       "       sales_ewm_alpha_0.5_lag_13  sales_ewm_alpha_0.5_lag_26  \\\n",
       "0                             NaN                         NaN   \n",
       "15                            NaN                         NaN   \n",
       "938                           NaN                         NaN   \n",
       "1861                          NaN                         NaN   \n",
       "2784                          NaN                         NaN   \n",
       "...                           ...                         ...   \n",
       "32975                    0.064542                    0.730053   \n",
       "33701                   14.054289                   24.738254   \n",
       "34349                   22.447299                   16.270752   \n",
       "35018                    8.412715                    9.960041   \n",
       "35343                    2.182411                    4.307860   \n",
       "\n",
       "       sales_ewm_alpha_0.5_lag_52  sales_ewm_alpha_0.5_lag_108  \\\n",
       "0                             NaN                          NaN   \n",
       "15                            NaN                          NaN   \n",
       "938                           NaN                          NaN   \n",
       "1861                          NaN                          NaN   \n",
       "2784                          NaN                          NaN   \n",
       "...                           ...                          ...   \n",
       "32975                   10.810018                     7.305512   \n",
       "33701                   69.526814                    22.412348   \n",
       "34349                   69.010783                          NaN   \n",
       "35018                   37.603911                    11.000000   \n",
       "35343                   34.571429                          NaN   \n",
       "\n",
       "       sales_ewm_alpha_0.4_lag_4  sales_ewm_alpha_0.4_lag_13  \\\n",
       "0                            NaN                         NaN   \n",
       "15                           NaN                         NaN   \n",
       "938                          NaN                         NaN   \n",
       "1861                         NaN                         NaN   \n",
       "2784                         NaN                         NaN   \n",
       "...                          ...                         ...   \n",
       "32975                   0.002773                    0.165118   \n",
       "33701                  19.587649                   16.480274   \n",
       "34349                   2.957552                   25.229022   \n",
       "35018                   1.233186                    9.665820   \n",
       "35343                   2.542832                    3.265311   \n",
       "\n",
       "       sales_ewm_alpha_0.4_lag_26  sales_ewm_alpha_0.4_lag_52  \\\n",
       "0                             NaN                         NaN   \n",
       "15                            NaN                         NaN   \n",
       "938                           NaN                         NaN   \n",
       "1861                          NaN                         NaN   \n",
       "2784                          NaN                         NaN   \n",
       "...                           ...                         ...   \n",
       "32975                    1.008419                    8.832905   \n",
       "33701                   25.887072                   66.681642   \n",
       "34349                   16.198154                   68.006972   \n",
       "35018                   10.106544                   36.894276   \n",
       "35343                    4.933146                   35.020408   \n",
       "\n",
       "       sales_ewm_alpha_0.4_lag_108  \n",
       "0                              NaN  \n",
       "15                             NaN  \n",
       "938                            NaN  \n",
       "1861                           NaN  \n",
       "2784                           NaN  \n",
       "...                            ...  \n",
       "32975                     7.717980  \n",
       "33701                    22.411728  \n",
       "34349                          NaN  \n",
       "35018                    11.000000  \n",
       "35343                          NaN  \n",
       "\n",
       "[35344 rows x 54 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = [0.9, 0.8, 0.7, 0.5, 0.4]\n",
    "lags = [4, 13, 26, 52, 108]\n",
    "ewm_features(train, alphas, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['PROMO1_FLAG', 'AUTORIZATION_FLAG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_rk</th>\n",
       "      <th>store_location_rk</th>\n",
       "      <th>period_start_dt</th>\n",
       "      <th>demand</th>\n",
       "      <th>PRICE_REGULAR</th>\n",
       "      <th>PRICE_AFTER_DISC</th>\n",
       "      <th>store_location_lvl_rk4</th>\n",
       "      <th>month</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_4</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_13</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_26</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_52</th>\n",
       "      <th>sales_ewm_alpha_0.4_lag_108</th>\n",
       "      <th>PROMO1_FLAG_0.0</th>\n",
       "      <th>PROMO1_FLAG_1.0</th>\n",
       "      <th>PROMO1_FLAG_2.0</th>\n",
       "      <th>AUTORIZATION_FLAG_0.0</th>\n",
       "      <th>AUTORIZATION_FLAG_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.451267</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454746</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>17.451267</td>\n",
       "      <td>9.794074</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.016786</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>944</td>\n",
       "      <td>17.451267</td>\n",
       "      <td>13.337798</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.256396</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>1873</td>\n",
       "      <td>17.451267</td>\n",
       "      <td>13.883093</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.016786</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>2802</td>\n",
       "      <td>17.451267</td>\n",
       "      <td>18.954918</td>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454746</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32975</th>\n",
       "      <td>33150</td>\n",
       "      <td>8.431571</td>\n",
       "      <td>15.236060</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>69.300000</td>\n",
       "      <td>14.454746</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.165118</td>\n",
       "      <td>1.008419</td>\n",
       "      <td>8.832905</td>\n",
       "      <td>7.717980</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33701</th>\n",
       "      <td>33881</td>\n",
       "      <td>8.431571</td>\n",
       "      <td>23.014288</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.021429</td>\n",
       "      <td>84.714286</td>\n",
       "      <td>16.256396</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>19.587649</td>\n",
       "      <td>16.480274</td>\n",
       "      <td>25.887072</td>\n",
       "      <td>66.681642</td>\n",
       "      <td>22.411728</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34349</th>\n",
       "      <td>34535</td>\n",
       "      <td>8.431571</td>\n",
       "      <td>24.320586</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>12.016786</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.957552</td>\n",
       "      <td>25.229022</td>\n",
       "      <td>16.198154</td>\n",
       "      <td>68.006972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>35210</td>\n",
       "      <td>8.431571</td>\n",
       "      <td>19.225742</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>8.164348</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.233186</td>\n",
       "      <td>9.665820</td>\n",
       "      <td>10.106544</td>\n",
       "      <td>36.894276</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35343</th>\n",
       "      <td>35541</td>\n",
       "      <td>8.431571</td>\n",
       "      <td>7.086761</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141.430000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>15.690604</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.542832</td>\n",
       "      <td>3.265311</td>\n",
       "      <td>4.933146</td>\n",
       "      <td>35.020408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35344 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  product_rk  store_location_rk period_start_dt  demand  \\\n",
       "0          0   17.451267          55.666667      2016-12-19    29.0   \n",
       "15        15   17.451267           9.794074      2016-12-19    50.0   \n",
       "938      944   17.451267          13.337798      2016-12-19    38.0   \n",
       "1861    1873   17.451267          13.883093      2016-12-19    39.0   \n",
       "2784    2802   17.451267          18.954918      2016-12-19   105.0   \n",
       "...      ...         ...                ...             ...     ...   \n",
       "32975  33150    8.431571          15.236060      2019-12-30     NaN   \n",
       "33701  33881    8.431571          23.014288      2019-12-30     NaN   \n",
       "34349  34535    8.431571          24.320586      2019-12-30     NaN   \n",
       "35018  35210    8.431571          19.225742      2019-12-30     NaN   \n",
       "35343  35541    8.431571           7.086761      2019-12-30     NaN   \n",
       "\n",
       "       PRICE_REGULAR  PRICE_AFTER_DISC  store_location_lvl_rk4  month  \\\n",
       "0                NaN               NaN               14.454746     12   \n",
       "15               NaN               NaN               12.016786     12   \n",
       "938              NaN               NaN               16.256396     12   \n",
       "1861             NaN               NaN               12.016786     12   \n",
       "2784             NaN               NaN               14.454746     12   \n",
       "...              ...               ...                     ...    ...   \n",
       "32975      99.000000         69.300000               14.454746     12   \n",
       "33701     121.021429         84.714286               16.256396     12   \n",
       "34349     141.430000         99.000000               12.016786     12   \n",
       "35018     141.430000         99.000000                8.164348     12   \n",
       "35343     141.430000         99.000000               15.690604     12   \n",
       "\n",
       "       week_of_month  ...  sales_ewm_alpha_0.4_lag_4  \\\n",
       "0                  2  ...                        NaN   \n",
       "15                 2  ...                        NaN   \n",
       "938                2  ...                        NaN   \n",
       "1861               2  ...                        NaN   \n",
       "2784               2  ...                        NaN   \n",
       "...              ...  ...                        ...   \n",
       "32975              4  ...                   0.002773   \n",
       "33701              4  ...                  19.587649   \n",
       "34349              4  ...                   2.957552   \n",
       "35018              4  ...                   1.233186   \n",
       "35343              4  ...                   2.542832   \n",
       "\n",
       "       sales_ewm_alpha_0.4_lag_13  sales_ewm_alpha_0.4_lag_26  \\\n",
       "0                             NaN                         NaN   \n",
       "15                            NaN                         NaN   \n",
       "938                           NaN                         NaN   \n",
       "1861                          NaN                         NaN   \n",
       "2784                          NaN                         NaN   \n",
       "...                           ...                         ...   \n",
       "32975                    0.165118                    1.008419   \n",
       "33701                   16.480274                   25.887072   \n",
       "34349                   25.229022                   16.198154   \n",
       "35018                    9.665820                   10.106544   \n",
       "35343                    3.265311                    4.933146   \n",
       "\n",
       "       sales_ewm_alpha_0.4_lag_52  sales_ewm_alpha_0.4_lag_108  \\\n",
       "0                             NaN                          NaN   \n",
       "15                            NaN                          NaN   \n",
       "938                           NaN                          NaN   \n",
       "1861                          NaN                          NaN   \n",
       "2784                          NaN                          NaN   \n",
       "...                           ...                          ...   \n",
       "32975                    8.832905                     7.717980   \n",
       "33701                   66.681642                    22.411728   \n",
       "34349                   68.006972                          NaN   \n",
       "35018                   36.894276                    11.000000   \n",
       "35343                   35.020408                          NaN   \n",
       "\n",
       "       PROMO1_FLAG_0.0  PROMO1_FLAG_1.0  PROMO1_FLAG_2.0  \\\n",
       "0                    0                0                0   \n",
       "15                   0                0                0   \n",
       "938                  0                0                0   \n",
       "1861                 0                0                0   \n",
       "2784                 0                0                0   \n",
       "...                ...              ...              ...   \n",
       "32975                0                1                0   \n",
       "33701                0                1                0   \n",
       "34349                0                1                0   \n",
       "35018                0                1                0   \n",
       "35343                0                1                0   \n",
       "\n",
       "       AUTORIZATION_FLAG_0.0  AUTORIZATION_FLAG_1.0  \n",
       "0                          0                      0  \n",
       "15                         0                      0  \n",
       "938                        0                      0  \n",
       "1861                       0                      0  \n",
       "2784                       0                      0  \n",
       "...                      ...                    ...  \n",
       "32975                      0                      1  \n",
       "33701                      0                      1  \n",
       "34349                      0                      1  \n",
       "35018                      0                      1  \n",
       "35343                      0                      1  \n",
       "\n",
       "[35344 rows x 57 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# использую mean encoding для категориальных переменных\n",
    "train['product_rk'] = train['product_rk'].map(train.groupby('product_rk')['demand'].mean())\n",
    "train['store_location_rk'] = train['store_location_rk'].map(train.groupby('store_location_rk')['demand'].mean())\n",
    "train['store_location_lvl_rk4'] = train['store_location_lvl_rk4'].map(train.groupby('store_location_lvl_rk4')['demand'].mean())\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# логарифмирую таргет, чтобы предсказания были положительными (спрос всегда больше 0)\n",
    "train['demand'] = np.log1p(train[\"demand\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in train.columns if col not in ['id', 'demand', 'period_start_dt']]\n",
    "\n",
    "train1 = train[train['period_start_dt'] < '2019-11-01'].copy()\n",
    "test1 = train[(train['period_start_dt'] >= '2019-11-01') & (train['period_start_dt'] < '2019-12-01')].copy()\n",
    "\n",
    "X_train = train1[cols]\n",
    "y_train = train1['demand']\n",
    "    \n",
    "X_val = test1[cols]\n",
    "y_val = test1['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подбираю гиперпараметры \n",
    "def objective(trial):\n",
    "    \n",
    "    param_grid = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'mae',\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': 5000,\n",
    "        'seed': 1,\n",
    "        'early_stopping_rounds': 200,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3), \n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.2, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.2, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    lgbtrain = lgb.Dataset(data=X_train, label=y_train, feature_name=cols)\n",
    "    lgbval = lgb.Dataset(data=X_val, label=y_val, reference=lgbtrain, feature_name=cols)\n",
    "    \n",
    "    model = lgb.train(param_grid, lgbtrain,\n",
    "                  valid_sets = lgbval,\n",
    "                  verbose_eval=100)\n",
    "    \n",
    "    y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    loss = mean_absolute_error(np.expm1(y_val), np.expm1(y_pred_val))\n",
    "#     loss = mean_absolute_error(y_val, y_pred_val)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:48:53,229]\u001b[0m A new study created in memory with name: LGBM\u001b[0m\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's l1: 0.639072\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:48:55,408]\u001b[0m Trial 0 finished with value: 4.26204549980148 and parameters: {'learning_rate': 0.2316445726306999, 'lambda_l1': 1.3578235818365787e-08, 'lambda_l2': 3.658223403074413e-07, 'num_leaves': 248, 'feature_fraction': 0.7579593519037879, 'bagging_fraction': 0.22668350100045834, 'bagging_freq': 2, 'min_child_samples': 91}. Best is trial 0 with value: 4.26204549980148.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's l1: 0.624014\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's l1: 0.575638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.525003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:48:57,235]\u001b[0m Trial 1 finished with value: 3.8635940956372044 and parameters: {'learning_rate': 0.2920483756052354, 'lambda_l1': 2.8280807912977065e-05, 'lambda_l2': 1.0616023427874725e-06, 'num_leaves': 106, 'feature_fraction': 0.2753781315807533, 'bagging_fraction': 0.722621965810659, 'bagging_freq': 1, 'min_child_samples': 60}. Best is trial 1 with value: 3.8635940956372044.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.534899\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's l1: 0.50262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008846 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.500766\n",
      "[200]\tvalid_0's l1: 0.49673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:00,196]\u001b[0m Trial 2 finished with value: 3.82245406039262 and parameters: {'learning_rate': 0.18672407702297858, 'lambda_l1': 9.747201152723555e-05, 'lambda_l2': 1.3683932176755773e-08, 'num_leaves': 101, 'feature_fraction': 0.5138052511481854, 'bagging_fraction': 0.511879442918459, 'bagging_freq': 2, 'min_child_samples': 42}. Best is trial 2 with value: 3.82245406039262.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 0.484575\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.513331\n",
      "[200]\tvalid_0's l1: 0.514073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:03,609]\u001b[0m Trial 3 finished with value: 3.958066067141928 and parameters: {'learning_rate': 0.05478625437994798, 'lambda_l1': 0.0015384583710357942, 'lambda_l2': 7.853470004077462, 'num_leaves': 102, 'feature_fraction': 0.5739236668501224, 'bagging_fraction': 0.4327269426912792, 'bagging_freq': 7, 'min_child_samples': 49}. Best is trial 2 with value: 3.82245406039262.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's l1: 0.502789\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.580048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:05,220]\u001b[0m Trial 4 finished with value: 4.163963669515241 and parameters: {'learning_rate': 0.2720288340234179, 'lambda_l1': 3.4497907347489836e-08, 'lambda_l2': 0.0115360013249628, 'num_leaves': 85, 'feature_fraction': 0.2749042328187481, 'bagging_fraction': 0.7137437930841886, 'bagging_freq': 5, 'min_child_samples': 78}. Best is trial 2 with value: 3.82245406039262.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.577078\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's l1: 0.551109\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.509864\n",
      "[200]\tvalid_0's l1: 0.504426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:11,711]\u001b[0m Trial 5 finished with value: 3.9000019784140942 and parameters: {'learning_rate': 0.09754649679183364, 'lambda_l1': 1.1626965247733616e-06, 'lambda_l2': 2.8359069796973414e-07, 'num_leaves': 168, 'feature_fraction': 0.9819894200132329, 'bagging_fraction': 0.6843843773961119, 'bagging_freq': 5, 'min_child_samples': 52}. Best is trial 2 with value: 3.82245406039262.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's l1: 0.497713\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.503923\n",
      "[200]\tvalid_0's l1: 0.516552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:19,839]\u001b[0m Trial 6 finished with value: 3.791792346471141 and parameters: {'learning_rate': 0.15432908437543794, 'lambda_l1': 1.4881316295322895e-05, 'lambda_l2': 2.607083490757928e-06, 'num_leaves': 225, 'feature_fraction': 0.9746692382209605, 'bagging_fraction': 0.817406178419416, 'bagging_freq': 5, 'min_child_samples': 60}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 0.477133\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.571677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:22,874]\u001b[0m Trial 7 finished with value: 4.200248421509766 and parameters: {'learning_rate': 0.29041795454512837, 'lambda_l1': 5.798033926847235e-07, 'lambda_l2': 0.9941005304633332, 'num_leaves': 240, 'feature_fraction': 0.2023782793122548, 'bagging_fraction': 0.6972199907091807, 'bagging_freq': 7, 'min_child_samples': 23}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.575865\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's l1: 0.543876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.643399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:25,198]\u001b[0m Trial 8 finished with value: 4.112263085746873 and parameters: {'learning_rate': 0.2721323913671505, 'lambda_l1': 0.11516039974582958, 'lambda_l2': 0.005071544164116637, 'num_leaves': 90, 'feature_fraction': 0.47631091207396237, 'bagging_fraction': 0.5395843405875693, 'bagging_freq': 5, 'min_child_samples': 9}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.65281\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's l1: 0.550269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's l1: 0.646311\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:27,760]\u001b[0m Trial 9 finished with value: 4.409977721220526 and parameters: {'learning_rate': 0.2663080451263154, 'lambda_l1': 4.9328222089408677e-08, 'lambda_l2': 2.2150927629093122e-07, 'num_leaves': 230, 'feature_fraction': 0.8873475723373812, 'bagging_fraction': 0.22270686581304613, 'bagging_freq': 2, 'min_child_samples': 68}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's l1: 0.642436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's l1: 0.614603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.592539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:29,253]\u001b[0m Trial 10 finished with value: 3.836353696782741 and parameters: {'learning_rate': 0.13493762583965505, 'lambda_l1': 8.071891145727449, 'lambda_l2': 5.7012254380429755e-05, 'num_leaves': 19, 'feature_fraction': 0.7456745188174777, 'bagging_fraction': 0.9402962794108517, 'bagging_freq': 4, 'min_child_samples': 34}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.606426\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 0.491417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.609339\n",
      "[200]\tvalid_0's l1: 0.618705\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's l1: 0.523725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:32,644]\u001b[0m Trial 11 finished with value: 4.066574189623528 and parameters: {'learning_rate': 0.20450377062761582, 'lambda_l1': 0.000494094425192292, 'lambda_l2': 2.1772154598140646e-05, 'num_leaves': 172, 'feature_fraction': 0.45091204113145245, 'bagging_fraction': 0.9322411477793898, 'bagging_freq': 3, 'min_child_samples': 34}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.770451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:34,250]\u001b[0m Trial 12 finished with value: 4.0348995744703915 and parameters: {'learning_rate': 0.17381469295655125, 'lambda_l1': 2.5122491752156307e-05, 'lambda_l2': 1.2603358543970776e-08, 'num_leaves': 37, 'feature_fraction': 0.6759451065936729, 'bagging_fraction': 0.4747519679027902, 'bagging_freq': 4, 'min_child_samples': 42}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.764775\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l1: 0.510595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's l1: 0.552215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's l1: 0.559081\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:36,919]\u001b[0m Trial 13 finished with value: 4.085614467340912 and parameters: {'learning_rate': 0.13768852036886703, 'lambda_l1': 0.017719267049860536, 'lambda_l2': 1.4726412534320251e-08, 'num_leaves': 190, 'feature_fraction': 0.44034520417670614, 'bagging_fraction': 0.3590942531275958, 'bagging_freq': 6, 'min_child_samples': 77}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's l1: 0.522133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.571439\n",
      "[200]\tvalid_0's l1: 0.572853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:40,874]\u001b[0m Trial 14 finished with value: 3.926123032003966 and parameters: {'learning_rate': 0.18444762187913855, 'lambda_l1': 7.857226903781447e-06, 'lambda_l2': 6.655284567506834e-06, 'num_leaves': 147, 'feature_fraction': 0.5779370234053371, 'bagging_fraction': 0.8503774313090591, 'bagging_freq': 3, 'min_child_samples': 99}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l1: 0.501297\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's l1: 0.569043\n",
      "[200]\tvalid_0's l1: 0.515384\n",
      "[300]\tvalid_0's l1: 0.512824\n",
      "[400]\tvalid_0's l1: 0.525091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:47,195]\u001b[0m Trial 15 finished with value: 3.9916012139719403 and parameters: {'learning_rate': 0.013432757350781338, 'lambda_l1': 0.007554321545528455, 'lambda_l2': 0.0011685227121613871, 'num_leaves': 57, 'feature_fraction': 0.873175048751345, 'bagging_fraction': 0.5944897672698553, 'bagging_freq': 1, 'min_child_samples': 62}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's l1: 0.503926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.535327\n",
      "[200]\tvalid_0's l1: 0.531386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:55,156]\u001b[0m Trial 16 finished with value: 3.840157737708538 and parameters: {'learning_rate': 0.11453231896606629, 'lambda_l1': 9.151526278129117e-05, 'lambda_l2': 1.0118498744026587e-08, 'num_leaves': 210, 'feature_fraction': 0.9798611815966606, 'bagging_fraction': 0.8626144024684088, 'bagging_freq': 3, 'min_child_samples': 18}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l1: 0.479452\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.599007\n",
      "[200]\tvalid_0's l1: 0.614279\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's l1: 0.529518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:49:58,736]\u001b[0m Trial 17 finished with value: 4.127529609723254 and parameters: {'learning_rate': 0.22326687707469686, 'lambda_l1': 1.5649269140891748e-06, 'lambda_l2': 0.00016303596977467627, 'num_leaves': 129, 'feature_fraction': 0.6664989306806873, 'bagging_fraction': 0.8181491619321613, 'bagging_freq': 6, 'min_child_samples': 38}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007973 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.593613\n",
      "[200]\tvalid_0's l1: 0.588221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:00,624]\u001b[0m Trial 18 finished with value: 4.154361086501548 and parameters: {'learning_rate': 0.0820055759038639, 'lambda_l1': 0.32809697906061097, 'lambda_l2': 3.1586039830653074e-06, 'num_leaves': 60, 'feature_fraction': 0.37937210036455904, 'bagging_fraction': 0.5839084784894975, 'bagging_freq': 2, 'min_child_samples': 69}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's l1: 0.530197\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.539312\n",
      "[200]\tvalid_0's l1: 0.547112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:05,983]\u001b[0m Trial 19 finished with value: 3.9226046160225136 and parameters: {'learning_rate': 0.16148541484650772, 'lambda_l1': 0.0002620892681240597, 'lambda_l2': 7.834670698771793e-08, 'num_leaves': 139, 'feature_fraction': 0.8387885147608498, 'bagging_fraction': 0.3447506573826702, 'bagging_freq': 4, 'min_child_samples': 25}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's l1: 0.512015\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.581189\n",
      "[200]\tvalid_0's l1: 0.589907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:10,671]\u001b[0m Trial 20 finished with value: 3.9583534649068217 and parameters: {'learning_rate': 0.20550765614179978, 'lambda_l1': 0.002689490126795108, 'lambda_l2': 3.082390044561065e-06, 'num_leaves': 213, 'feature_fraction': 0.5221949140616752, 'bagging_fraction': 0.7911205450163026, 'bagging_freq': 6, 'min_child_samples': 48}. Best is trial 6 with value: 3.791792346471141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 0.517309\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.52187\n",
      "[200]\tvalid_0's l1: 0.60138\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's l1: 0.492174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:11,625]\u001b[0m Trial 21 finished with value: 3.721312330953892 and parameters: {'learning_rate': 0.13280076255848866, 'lambda_l1': 0.825279803780157, 'lambda_l2': 0.00015551132572978645, 'num_leaves': 5, 'feature_fraction': 0.7578413597387414, 'bagging_fraction': 0.9644935684765327, 'bagging_freq': 4, 'min_child_samples': 35}. Best is trial 21 with value: 3.721312330953892.\u001b[0m\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.575816\n",
      "[200]\tvalid_0's l1: 0.584973\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's l1: 0.480417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:13,052]\u001b[0m Trial 22 finished with value: 3.691908868283346 and parameters: {'learning_rate': 0.13887689801014513, 'lambda_l1': 8.707224720817273, 'lambda_l2': 0.0006998717710169579, 'num_leaves': 16, 'feature_fraction': 0.7622902580017142, 'bagging_fraction': 0.9708950542199323, 'bagging_freq': 5, 'min_child_samples': 57}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.541332\n",
      "[200]\tvalid_0's l1: 0.568739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:14,222]\u001b[0m Trial 23 finished with value: 3.7435962886784546 and parameters: {'learning_rate': 0.13531309721176, 'lambda_l1': 6.962779129243086, 'lambda_l2': 0.0006095456245555525, 'num_leaves': 7, 'feature_fraction': 0.7848605473720227, 'bagging_fraction': 0.9981440018523615, 'bagging_freq': 5, 'min_child_samples': 59}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's l1: 0.487004\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's l1: 0.479142\n",
      "[200]\tvalid_0's l1: 0.559295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:15,725]\u001b[0m Trial 24 finished with value: 3.7478562889918603 and parameters: {'learning_rate': 0.06788165583656619, 'lambda_l1': 6.098287679470665, 'lambda_l2': 0.04466209536524307, 'num_leaves': 10, 'feature_fraction': 0.7976252868009509, 'bagging_fraction': 0.9924132360894056, 'bagging_freq': 5, 'min_child_samples': 80}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's l1: 0.594618\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's l1: 0.479142\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's l1: 0.537058\n",
      "[200]\tvalid_0's l1: 0.554975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:16,444]\u001b[0m Trial 25 finished with value: 3.8309054503880016 and parameters: {'learning_rate': 0.11822066022467832, 'lambda_l1': 0.8326422385068835, 'lambda_l2': 0.0004466505721883067, 'num_leaves': 3, 'feature_fraction': 0.6863318097840245, 'bagging_fraction': 0.9847909977131678, 'bagging_freq': 6, 'min_child_samples': 56}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's l1: 0.517726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.556567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:18,310]\u001b[0m Trial 26 finished with value: 3.7210912585481077 and parameters: {'learning_rate': 0.14319389311410285, 'lambda_l1': 1.5408361337444185, 'lambda_l2': 0.11570742533659098, 'num_leaves': 33, 'feature_fraction': 0.7326565824623541, 'bagging_fraction': 0.9292965682794194, 'bagging_freq': 4, 'min_child_samples': 31}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.555581\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's l1: 0.474407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.495601\n",
      "[200]\tvalid_0's l1: 0.55308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:21,343]\u001b[0m Trial 27 finished with value: 3.8080481038462652 and parameters: {'learning_rate': 0.03748340264267232, 'lambda_l1': 0.8644835241465368, 'lambda_l2': 0.1345385167627051, 'num_leaves': 35, 'feature_fraction': 0.7110580838429968, 'bagging_fraction': 0.9285608567218567, 'bagging_freq': 4, 'min_child_samples': 8}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's l1: 0.48563\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.586747\n",
      "[200]\tvalid_0's l1: 0.580013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:24,317]\u001b[0m Trial 28 finished with value: 3.9311650336405193 and parameters: {'learning_rate': 0.10416407729128294, 'lambda_l1': 0.05080283810068673, 'lambda_l2': 0.004633230649616843, 'num_leaves': 64, 'feature_fraction': 0.6084607103319352, 'bagging_fraction': 0.9003281463353398, 'bagging_freq': 3, 'min_child_samples': 27}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's l1: 0.506484\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.567194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:26,159]\u001b[0m Trial 29 finished with value: 3.9210064960755298 and parameters: {'learning_rate': 0.15020085051773793, 'lambda_l1': 2.082487674348136, 'lambda_l2': 0.18360838295723558, 'num_leaves': 33, 'feature_fraction': 0.6392790597956275, 'bagging_fraction': 0.7681441941201659, 'bagging_freq': 4, 'min_child_samples': 19}. Best is trial 22 with value: 3.691908868283346.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.563355\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 0.505553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.649092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:28,736]\u001b[0m Trial 30 finished with value: 3.6869060884713596 and parameters: {'learning_rate': 0.24099603355292887, 'lambda_l1': 0.2131266427782563, 'lambda_l2': 8.164753183716365e-05, 'num_leaves': 48, 'feature_fraction': 0.7524274175918187, 'bagging_fraction': 0.9311992412920377, 'bagging_freq': 4, 'min_child_samples': 32}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.645289\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's l1: 0.472692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.601852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:30,301]\u001b[0m Trial 31 finished with value: 3.824769384026746 and parameters: {'learning_rate': 0.23439970232574733, 'lambda_l1': 0.1933542801317565, 'lambda_l2': 4.7297720324934e-05, 'num_leaves': 24, 'feature_fraction': 0.7366366292384973, 'bagging_fraction': 0.8624390005466506, 'bagging_freq': 4, 'min_child_samples': 30}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.587139\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l1: 0.480474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.575272\n",
      "[200]\tvalid_0's l1: 0.593589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:33,538]\u001b[0m Trial 32 finished with value: 3.966394709632445 and parameters: {'learning_rate': 0.09214659193963323, 'lambda_l1': 1.4870641094111243, 'lambda_l2': 0.00012035113001058024, 'num_leaves': 51, 'feature_fraction': 0.8246598118682085, 'bagging_fraction': 0.9046299558826548, 'bagging_freq': 3, 'min_child_samples': 44}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's l1: 0.504678\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.64166\n",
      "[200]\tvalid_0's l1: 0.663334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:36,922]\u001b[0m Trial 33 finished with value: 4.035342237104692 and parameters: {'learning_rate': 0.1696668019366296, 'lambda_l1': 0.08007238946595377, 'lambda_l2': 0.003382829000811011, 'num_leaves': 73, 'feature_fraction': 0.8906559318139271, 'bagging_fraction': 0.9362768454286106, 'bagging_freq': 4, 'min_child_samples': 35}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's l1: 0.508153\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.550977\n",
      "[200]\tvalid_0's l1: 0.571225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:39,384]\u001b[0m Trial 34 finished with value: 3.8648335531732334 and parameters: {'learning_rate': 0.1270018287488932, 'lambda_l1': 0.33960571930467703, 'lambda_l2': 1.3716685873437976e-05, 'num_leaves': 42, 'feature_fraction': 0.763860626157188, 'bagging_fraction': 0.7587471579547238, 'bagging_freq': 5, 'min_child_samples': 14}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's l1: 0.490562\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.586237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:40,917]\u001b[0m Trial 35 finished with value: 3.7316003972283056 and parameters: {'learning_rate': 0.24687383621788866, 'lambda_l1': 0.0261673692798905, 'lambda_l2': 0.022978332026491334, 'num_leaves': 23, 'feature_fraction': 0.9223174404087516, 'bagging_fraction': 0.6532258603394359, 'bagging_freq': 4, 'min_child_samples': 45}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.615575\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's l1: 0.478525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.608199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:44,242]\u001b[0m Trial 36 finished with value: 3.9593307252691443 and parameters: {'learning_rate': 0.19362209489273846, 'lambda_l1': 3.050589270857534, 'lambda_l2': 2.125823603382684, 'num_leaves': 77, 'feature_fraction': 0.7273811876948376, 'bagging_fraction': 0.8871886939044602, 'bagging_freq': 3, 'min_child_samples': 40}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.598689\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's l1: 0.504434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.588854\n",
      "[200]\tvalid_0's l1: 0.588765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:48,645]\u001b[0m Trial 37 finished with value: 3.9440449026226307 and parameters: {'learning_rate': 0.1495426076726166, 'lambda_l1': 0.5674938892282111, 'lambda_l2': 0.0007385639561779432, 'num_leaves': 114, 'feature_fraction': 0.7998523793248469, 'bagging_fraction': 0.9615608959124321, 'bagging_freq': 5, 'min_child_samples': 52}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.49362\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.603595\n",
      "[200]\tvalid_0's l1: 0.589619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:51,529]\u001b[0m Trial 38 finished with value: 4.024698348105951 and parameters: {'learning_rate': 0.11565913180803776, 'lambda_l1': 2.138052657907817, 'lambda_l2': 0.0020388086066767354, 'num_leaves': 48, 'feature_fraction': 0.6257007456800845, 'bagging_fraction': 0.8391561482958296, 'bagging_freq': 4, 'min_child_samples': 31}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's l1: 0.513094\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.546027\n",
      "[200]\tvalid_0's l1: 0.570239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:53,296]\u001b[0m Trial 39 finished with value: 3.7487273490722357 and parameters: {'learning_rate': 0.07681783277539789, 'lambda_l1': 0.008170037566386146, 'lambda_l2': 0.0002147176438287495, 'num_leaves': 22, 'feature_fraction': 0.5435554021324348, 'bagging_fraction': 0.9579328970542575, 'bagging_freq': 5, 'min_child_samples': 22}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's l1: 0.481302\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.692354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:54,335]\u001b[0m Trial 40 finished with value: 4.035691103014492 and parameters: {'learning_rate': 0.2983203119024269, 'lambda_l1': 0.1380064231436032, 'lambda_l2': 6.662121813128381, 'num_leaves': 10, 'feature_fraction': 0.8442624306198587, 'bagging_fraction': 0.7469560553747123, 'bagging_freq': 1, 'min_child_samples': 48}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.682288\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's l1: 0.540949\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.700573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:56,015]\u001b[0m Trial 41 finished with value: 3.8419118308214286 and parameters: {'learning_rate': 0.251319195034217, 'lambda_l1': 0.02927237219537595, 'lambda_l2': 0.02052082029454647, 'num_leaves': 25, 'feature_fraction': 0.9256735075885594, 'bagging_fraction': 0.6749933787323694, 'bagging_freq': 4, 'min_child_samples': 45}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.735631\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l1: 0.504685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.61905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:50:57,911]\u001b[0m Trial 42 finished with value: 3.8216056152757734 and parameters: {'learning_rate': 0.23808614393865532, 'lambda_l1': 3.0304813504901817, 'lambda_l2': 0.27679716928247716, 'num_leaves': 21, 'feature_fraction': 0.9236980321057559, 'bagging_fraction': 0.8950550824931611, 'bagging_freq': 3, 'min_child_samples': 38}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.619045\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's l1: 0.496558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015106 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.673304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:00,502]\u001b[0m Trial 43 finished with value: 4.059059186615206 and parameters: {'learning_rate': 0.21140784888210973, 'lambda_l1': 9.955177824937598, 'lambda_l2': 0.06560412257756247, 'num_leaves': 40, 'feature_fraction': 0.9318851198417445, 'bagging_fraction': 0.6418323493621816, 'bagging_freq': 4, 'min_child_samples': 29}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.675686\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's l1: 0.527861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.547984\n",
      "[200]\tvalid_0's l1: 0.570912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:04,010]\u001b[0m Trial 44 finished with value: 3.9113333422435237 and parameters: {'learning_rate': 0.2527159441638392, 'lambda_l1': 0.2723816260530168, 'lambda_l2': 0.012942781109873808, 'num_leaves': 88, 'feature_fraction': 0.687643105971024, 'bagging_fraction': 0.2989869506619279, 'bagging_freq': 5, 'min_child_samples': 65}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's l1: 0.521967\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.595954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:04,559]\u001b[0m Trial 45 finished with value: 3.871813750946955 and parameters: {'learning_rate': 0.2844886368940608, 'lambda_l1': 0.049668923833308974, 'lambda_l2': 0.6205402418444649, 'num_leaves': 3, 'feature_fraction': 0.7833210911975509, 'bagging_fraction': 0.4301094078363287, 'bagging_freq': 4, 'min_child_samples': 52}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.577637\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's l1: 0.514732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.630539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:07,717]\u001b[0m Trial 46 finished with value: 4.04547933845549 and parameters: {'learning_rate': 0.1820119687885592, 'lambda_l1': 1.046109953995944, 'lambda_l2': 0.029522749635689276, 'num_leaves': 69, 'feature_fraction': 0.8316576213696746, 'bagging_fraction': 0.9625745250888754, 'bagging_freq': 5, 'min_child_samples': 34}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.623161\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's l1: 0.520528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.574207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:09,372]\u001b[0m Trial 47 finished with value: 3.7686138583222464 and parameters: {'learning_rate': 0.16448756765775713, 'lambda_l1': 0.014236571279347224, 'lambda_l2': 5.0916793856349355e-05, 'num_leaves': 29, 'feature_fraction': 0.6518012318742606, 'bagging_fraction': 0.8022480374453509, 'bagging_freq': 3, 'min_child_samples': 56}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.577083\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 0.483432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.607525\n",
      "[200]\tvalid_0's l1: 0.604697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:13,364]\u001b[0m Trial 48 finished with value: 4.088367101800836 and parameters: {'learning_rate': 0.1449677626306005, 'lambda_l1': 0.0012552417163698487, 'lambda_l2': 0.010259445846667315, 'num_leaves': 105, 'feature_fraction': 0.7533965598789452, 'bagging_fraction': 0.7222626465831479, 'bagging_freq': 7, 'min_child_samples': 45}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's l1: 0.534549\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.561673\n",
      "[200]\tvalid_0's l1: 0.568159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:15,819]\u001b[0m Trial 49 finished with value: 3.915737780886175 and parameters: {'learning_rate': 0.21952017190443124, 'lambda_l1': 0.47842955220169403, 'lambda_l2': 0.001590272780511263, 'num_leaves': 51, 'feature_fraction': 0.7113235637935416, 'bagging_fraction': 0.5385775888789928, 'bagging_freq': 4, 'min_child_samples': 14}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's l1: 0.503478\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.543701\n",
      "[200]\tvalid_0's l1: 0.577238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:17,633]\u001b[0m Trial 50 finished with value: 3.882114148237408 and parameters: {'learning_rate': 0.10219343705289409, 'lambda_l1': 4.122660618317331, 'lambda_l2': 1.7482238840339768e-05, 'num_leaves': 14, 'feature_fraction': 0.8859594757208538, 'bagging_fraction': 0.8755518900160159, 'bagging_freq': 5, 'min_child_samples': 38}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's l1: 0.504345\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.542599\n",
      "[200]\tvalid_0's l1: 0.582559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:19,244]\u001b[0m Trial 51 finished with value: 3.8163095394206747 and parameters: {'learning_rate': 0.13298059782326913, 'lambda_l1': 7.896111611501555, 'lambda_l2': 0.000543205140548943, 'num_leaves': 14, 'feature_fraction': 0.7817930843300839, 'bagging_fraction': 0.9980088644203833, 'bagging_freq': 6, 'min_child_samples': 73}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's l1: 0.488462\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.556722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:20,088]\u001b[0m Trial 52 finished with value: 3.8899695953573827 and parameters: {'learning_rate': 0.12568708823199418, 'lambda_l1': 1.5727289673265108, 'lambda_l2': 9.702421459181285e-05, 'num_leaves': 3, 'feature_fraction': 0.863798492607627, 'bagging_fraction': 0.9341947192641844, 'bagging_freq': 5, 'min_child_samples': 60}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.573363\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's l1: 0.52341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.609152\n",
      "[200]\tvalid_0's l1: 0.596584\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's l1: 0.501397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:22,123]\u001b[0m Trial 53 finished with value: 3.968393329161306 and parameters: {'learning_rate': 0.14346323905569075, 'lambda_l1': 0.11235889703002659, 'lambda_l2': 0.00026843130381638423, 'num_leaves': 31, 'feature_fraction': 0.8104998634012477, 'bagging_fraction': 0.9692343532636989, 'bagging_freq': 5, 'min_child_samples': 56}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.666432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:24,572]\u001b[0m Trial 54 finished with value: 4.006568389482284 and parameters: {'learning_rate': 0.19436015758438951, 'lambda_l1': 3.068656294566403, 'lambda_l2': 1.1781798627596806e-06, 'num_leaves': 44, 'feature_fraction': 0.7573621934894019, 'bagging_fraction': 0.9988547185857796, 'bagging_freq': 4, 'min_child_samples': 66}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.666754\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.507266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.566667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:25,910]\u001b[0m Trial 55 finished with value: 3.737546413970344 and parameters: {'learning_rate': 0.174433842598829, 'lambda_l1': 0.4621908771045478, 'lambda_l2': 0.008070728716319475, 'num_leaves': 14, 'feature_fraction': 0.9480440509057522, 'bagging_fraction': 0.917340707104922, 'bagging_freq': 6, 'min_child_samples': 42}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.57041\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's l1: 0.476688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.594139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:27,523]\u001b[0m Trial 56 finished with value: 3.8603675322518938 and parameters: {'learning_rate': 0.25160867044220775, 'lambda_l1': 0.21268730890854043, 'lambda_l2': 0.007183160626948505, 'num_leaves': 17, 'feature_fraction': 0.9973402118265692, 'bagging_fraction': 0.8254922302758145, 'bagging_freq': 6, 'min_child_samples': 41}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.590953\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's l1: 0.492615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.552683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:30,394]\u001b[0m Trial 57 finished with value: 3.977200408492475 and parameters: {'learning_rate': 0.273544862343789, 'lambda_l1': 0.6917718754238925, 'lambda_l2': 0.10757567852779866, 'num_leaves': 58, 'feature_fraction': 0.9474509153221115, 'bagging_fraction': 0.9163871705314219, 'bagging_freq': 7, 'min_child_samples': 49}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.55779\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's l1: 0.510203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.694632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:32,608]\u001b[0m Trial 58 finished with value: 4.039379783709246 and parameters: {'learning_rate': 0.17617310995501417, 'lambda_l1': 0.025816816124865145, 'lambda_l2': 0.002761982958990709, 'num_leaves': 34, 'feature_fraction': 0.9590446143476963, 'bagging_fraction': 0.8737700815144316, 'bagging_freq': 6, 'min_child_samples': 33}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.705634\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's l1: 0.518386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.576899\n",
      "[200]\tvalid_0's l1: 0.593401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:34,394]\u001b[0m Trial 59 finished with value: 3.7548849703169935 and parameters: {'learning_rate': 0.16147177779787578, 'lambda_l1': 0.00588759775651217, 'lambda_l2': 0.40776597543878645, 'num_leaves': 17, 'feature_fraction': 0.8985746481140799, 'bagging_fraction': 0.8413613131933382, 'bagging_freq': 3, 'min_child_samples': 24}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's l1: 0.482468\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.61436\n",
      "[200]\tvalid_0's l1: 0.613293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:38,550]\u001b[0m Trial 60 finished with value: 4.029896264002981 and parameters: {'learning_rate': 0.2654999268547948, 'lambda_l1': 1.5989746493505004e-08, 'lambda_l2': 0.017411152410890442, 'num_leaves': 154, 'feature_fraction': 0.5807764872829769, 'bagging_fraction': 0.6384729308374846, 'bagging_freq': 4, 'min_child_samples': 44}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's l1: 0.522925\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.580613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:39,368]\u001b[0m Trial 61 finished with value: 3.8769229469194624 and parameters: {'learning_rate': 0.1570282556125032, 'lambda_l1': 1.29800872638117, 'lambda_l2': 0.000696482987248074, 'num_leaves': 3, 'feature_fraction': 0.849709140337945, 'bagging_fraction': 0.95847369311597, 'bagging_freq': 5, 'min_child_samples': 60}. Best is trial 30 with value: 3.6869060884713596.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.565592\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's l1: 0.526963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.53188\n",
      "[200]\tvalid_0's l1: 0.578259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:40,600]\u001b[0m Trial 62 finished with value: 3.6002945885329267 and parameters: {'learning_rate': 0.1338962411650117, 'lambda_l1': 0.48074511048310864, 'lambda_l2': 0.0011040836215668386, 'num_leaves': 9, 'feature_fraction': 0.7102293153172105, 'bagging_fraction': 0.9176552911274246, 'bagging_freq': 4, 'min_child_samples': 37}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's l1: 0.471655\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\tvalid_0's l1: 0.62478\n",
      "[200]\tvalid_0's l1: 0.633382\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's l1: 0.507464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:42,279]\u001b[0m Trial 63 finished with value: 3.953266958761347 and parameters: {'learning_rate': 0.11124507646308429, 'lambda_l1': 0.06356046041462707, 'lambda_l2': 0.03978869392371414, 'num_leaves': 26, 'feature_fraction': 0.6968144545758019, 'bagging_fraction': 0.9196333500291805, 'bagging_freq': 4, 'min_child_samples': 36}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.576778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:43,760]\u001b[0m Trial 64 finished with value: 4.011978193305117 and parameters: {'learning_rate': 0.09115658080071454, 'lambda_l1': 0.4243564410378999, 'lambda_l2': 0.0013123271140278734, 'num_leaves': 14, 'feature_fraction': 0.658434921029873, 'bagging_fraction': 0.7921971172550643, 'bagging_freq': 4, 'min_child_samples': 28}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.595367\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's l1: 0.508735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.47998\n",
      "[200]\tvalid_0's l1: 0.483045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:46,404]\u001b[0m Trial 65 finished with value: 3.619545251805835 and parameters: {'learning_rate': 0.19529335440672432, 'lambda_l1': 4.540649715305205, 'lambda_l2': 0.006539757993861473, 'num_leaves': 40, 'feature_fraction': 0.7226027569975387, 'bagging_fraction': 0.9455359170189015, 'bagging_freq': 3, 'min_child_samples': 32}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's l1: 0.466722\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.590483\n",
      "[200]\tvalid_0's l1: 0.584646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:50,216]\u001b[0m Trial 66 finished with value: 3.9288488474194025 and parameters: {'learning_rate': 0.12443202059266915, 'lambda_l1': 4.1540018156672, 'lambda_l2': 3.094216593412461e-05, 'num_leaves': 81, 'feature_fraction': 0.7154303201537939, 'bagging_fraction': 0.9717315841495625, 'bagging_freq': 2, 'min_child_samples': 20}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's l1: 0.50091\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.578064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:52,497]\u001b[0m Trial 67 finished with value: 3.76829337465034 and parameters: {'learning_rate': 0.22193678756544183, 'lambda_l1': 5.061973877339085, 'lambda_l2': 9.318861482177805e-05, 'num_leaves': 42, 'feature_fraction': 0.7269746540454445, 'bagging_fraction': 0.945131398683761, 'bagging_freq': 3, 'min_child_samples': 26}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.564121\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l1: 0.471467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.69519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:54,881]\u001b[0m Trial 68 finished with value: 3.961280975444733 and parameters: {'learning_rate': 0.14020328629886086, 'lambda_l1': 0.1604342335087703, 'lambda_l2': 0.00029135006962685366, 'num_leaves': 55, 'feature_fraction': 0.605593338099988, 'bagging_fraction': 0.8812603035867272, 'bagging_freq': 3, 'min_child_samples': 32}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.695813\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 0.51073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.584649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:51:56,714]\u001b[0m Trial 69 finished with value: 3.99197783333361 and parameters: {'learning_rate': 0.19067166712342157, 'lambda_l1': 2.0977827566313816e-07, 'lambda_l2': 0.0688167377578656, 'num_leaves': 67, 'feature_fraction': 0.3432827025102498, 'bagging_fraction': 0.857954548226096, 'bagging_freq': 2, 'min_child_samples': 40}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.577526\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.523742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.595806\n",
      "[200]\tvalid_0's l1: 0.595741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:00,504]\u001b[0m Trial 70 finished with value: 3.8736998780471175 and parameters: {'learning_rate': 0.20794570091992515, 'lambda_l1': 1.0406570093749083, 'lambda_l2': 9.016763360121336e-06, 'num_leaves': 95, 'feature_fraction': 0.7467090819667493, 'bagging_fraction': 0.4745397648360874, 'bagging_freq': 4, 'min_child_samples': 36}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 0.491091\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.652542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:02,579]\u001b[0m Trial 71 finished with value: 3.893284672628687 and parameters: {'learning_rate': 0.15086392715629796, 'lambda_l1': 0.5711202361788266, 'lambda_l2': 0.0080712542007875, 'num_leaves': 34, 'feature_fraction': 0.8139981909243783, 'bagging_fraction': 0.9115036946966775, 'bagging_freq': 4, 'min_child_samples': 46}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.686859\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.497244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.657046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:03,713]\u001b[0m Trial 72 finished with value: 3.921601120891209 and parameters: {'learning_rate': 0.17011248892581338, 'lambda_l1': 2.0396117784968912, 'lambda_l2': 0.004596665389382658, 'num_leaves': 10, 'feature_fraction': 0.6935533389778324, 'bagging_fraction': 0.929901274067368, 'bagging_freq': 4, 'min_child_samples': 30}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.625997\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 0.51201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.618674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:05,375]\u001b[0m Trial 73 finished with value: 3.793843321132459 and parameters: {'learning_rate': 0.24281180672483754, 'lambda_l1': 0.2989945341696471, 'lambda_l2': 0.0029754252595847724, 'num_leaves': 24, 'feature_fraction': 0.7709730751320923, 'bagging_fraction': 0.8959784721474813, 'bagging_freq': 3, 'min_child_samples': 42}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.606838\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l1: 0.482346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.492791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:07,117]\u001b[0m Trial 74 finished with value: 3.737605546290021 and parameters: {'learning_rate': 0.19841169523798438, 'lambda_l1': 0.11148871131476398, 'lambda_l2': 0.001101592066909529, 'num_leaves': 37, 'feature_fraction': 0.6288081329072392, 'bagging_fraction': 0.9754307737385957, 'bagging_freq': 4, 'min_child_samples': 49}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.500991\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's l1: 0.474478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's l1: 0.545976\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's l1: 0.54082\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:13,449]\u001b[0m Trial 75 finished with value: 3.8958190114081463 and parameters: {'learning_rate': 0.1323335145972696, 'lambda_l1': 9.49610968613154, 'lambda_l2': 0.00015002364531756305, 'num_leaves': 255, 'feature_fraction': 0.6701745798144586, 'bagging_fraction': 0.9458884725112113, 'bagging_freq': 4, 'min_child_samples': 37}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's l1: 0.490692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.645868\n",
      "[200]\tvalid_0's l1: 0.654425\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.490051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:16,131]\u001b[0m Trial 76 finished with value: 3.895797043328613 and parameters: {'learning_rate': 0.1825833326680471, 'lambda_l1': 2.22792634637966, 'lambda_l2': 0.00042346353056844026, 'num_leaves': 47, 'feature_fraction': 0.9068781412136672, 'bagging_fraction': 0.9015671284517625, 'bagging_freq': 6, 'min_child_samples': 40}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.525253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:17,855]\u001b[0m Trial 77 finished with value: 3.8937985410925293 and parameters: {'learning_rate': 0.23024477408685198, 'lambda_l1': 0.8252979819622672, 'lambda_l2': 1.3127088694898759, 'num_leaves': 21, 'feature_fraction': 0.8665050771156836, 'bagging_fraction': 0.8197069238858544, 'bagging_freq': 3, 'min_child_samples': 84}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.564134\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 0.486124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.573512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:19,059]\u001b[0m Trial 78 finished with value: 3.7768778802842013 and parameters: {'learning_rate': 0.1580777987566023, 'lambda_l1': 6.2480471410235525e-06, 'lambda_l2': 0.029943362873312614, 'num_leaves': 9, 'feature_fraction': 0.7388227665014445, 'bagging_fraction': 0.9767135231691138, 'bagging_freq': 5, 'min_child_samples': 32}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.561962\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's l1: 0.488535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.582483\n",
      "[200]\tvalid_0's l1: 0.583802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:25,837]\u001b[0m Trial 79 finished with value: 4.1234279853732945 and parameters: {'learning_rate': 0.12085620800999973, 'lambda_l1': 4.660588735020749, 'lambda_l2': 0.01621474408267007, 'num_leaves': 191, 'feature_fraction': 0.7958424109712464, 'bagging_fraction': 0.9372085651062325, 'bagging_freq': 5, 'min_child_samples': 54}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's l1: 0.512032\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.588955\n",
      "[200]\tvalid_0's l1: 0.588856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:30,379]\u001b[0m Trial 80 finished with value: 3.9001069874203815 and parameters: {'learning_rate': 0.17452640883427611, 'lambda_l1': 0.00011467430144511924, 'lambda_l2': 0.0009285115500069817, 'num_leaves': 122, 'feature_fraction': 0.7709859422422392, 'bagging_fraction': 0.853404120195177, 'bagging_freq': 4, 'min_child_samples': 23}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l1: 0.491295\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.576889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:32,432]\u001b[0m Trial 81 finished with value: 3.836316872385124 and parameters: {'learning_rate': 0.11083912252620162, 'lambda_l1': 0.08090780286558201, 'lambda_l2': 0.0023548275589326205, 'num_leaves': 39, 'feature_fraction': 0.6310418245022008, 'bagging_fraction': 0.9795246562801375, 'bagging_freq': 4, 'min_child_samples': 43}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.577146\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.501936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.601417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:34,096]\u001b[0m Trial 82 finished with value: 3.9474255780188243 and parameters: {'learning_rate': 0.1964967236200316, 'lambda_l1': 0.040012081398633395, 'lambda_l2': 0.005496587841097649, 'num_leaves': 31, 'feature_fraction': 0.5196110757001503, 'bagging_fraction': 0.9565727178558467, 'bagging_freq': 4, 'min_child_samples': 39}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.63444\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.501823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.60555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:35,746]\u001b[0m Trial 83 finished with value: 3.818469240548558 and parameters: {'learning_rate': 0.2152345293837924, 'lambda_l1': 0.1045549101367253, 'lambda_l2': 0.0009479795251286351, 'num_leaves': 28, 'feature_fraction': 0.7066202554975944, 'bagging_fraction': 0.9191548475276785, 'bagging_freq': 4, 'min_child_samples': 51}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.59049\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's l1: 0.488477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.593816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:36,999]\u001b[0m Trial 84 finished with value: 3.9625148766552942 and parameters: {'learning_rate': 0.16682698222780884, 'lambda_l1': 0.19327637774980883, 'lambda_l2': 7.282224757496787e-05, 'num_leaves': 16, 'feature_fraction': 0.6437474213974292, 'bagging_fraction': 0.9877903981666606, 'bagging_freq': 4, 'min_child_samples': 48}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.590297\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l1: 0.499629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.588705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:38,218]\u001b[0m Trial 85 finished with value: 4.106425423499165 and parameters: {'learning_rate': 0.2609323484187882, 'lambda_l1': 0.37762775481313654, 'lambda_l2': 0.0017126109356345178, 'num_leaves': 38, 'feature_fraction': 0.22538851022577283, 'bagging_fraction': 0.20034163680291034, 'bagging_freq': 3, 'min_child_samples': 34}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.586953\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's l1: 0.54138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.530693\n",
      "[200]\tvalid_0's l1: 0.538772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:40,813]\u001b[0m Trial 86 finished with value: 3.850979874219027 and parameters: {'learning_rate': 0.15197670991153767, 'lambda_l1': 1.7062334859373065, 'lambda_l2': 0.00020530542162207335, 'num_leaves': 62, 'feature_fraction': 0.5567945528910954, 'bagging_fraction': 0.9471995103227767, 'bagging_freq': 5, 'min_child_samples': 50}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 0.485764\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's l1: 0.513772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:41,509]\u001b[0m Trial 87 finished with value: 3.6233106079132944 and parameters: {'learning_rate': 0.24354135590712017, 'lambda_l1': 0.6971663652198038, 'lambda_l2': 0.0004142233900724288, 'num_leaves': 8, 'feature_fraction': 0.6211432515113079, 'bagging_fraction': 0.28395736601419747, 'bagging_freq': 4, 'min_child_samples': 54}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.570885\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's l1: 0.479361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.581093\n",
      "[200]\tvalid_0's l1: 0.58514\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's l1: 0.502894"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:42,082]\u001b[0m Trial 88 finished with value: 3.8219655044956786 and parameters: {'learning_rate': 0.24376606328076172, 'lambda_l1': 0.672739740969862, 'lambda_l2': 2.627964791517359e-05, 'num_leaves': 6, 'feature_fraction': 0.4926223954272543, 'bagging_fraction': 0.30038211538346243, 'bagging_freq': 7, 'min_child_samples': 54}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.609878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:43,131]\u001b[0m Trial 89 finished with value: 3.9588554295063703 and parameters: {'learning_rate': 0.27882030385935785, 'lambda_l1': 2.8439323224984325, 'lambda_l2': 0.00036103433086543817, 'num_leaves': 10, 'feature_fraction': 0.68305469324546, 'bagging_fraction': 0.41035555486480213, 'bagging_freq': 3, 'min_child_samples': 28}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.590709\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l1: 0.520333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.61676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:44,666]\u001b[0m Trial 90 finished with value: 4.133285410402241 and parameters: {'learning_rate': 0.2298578374765392, 'lambda_l1': 1.3498978656791958, 'lambda_l2': 0.1312265868636227, 'num_leaves': 19, 'feature_fraction': 0.9656650493106536, 'bagging_fraction': 0.4937063224682558, 'bagging_freq': 4, 'min_child_samples': 63}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.605473\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's l1: 0.52678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.627746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:46,130]\u001b[0m Trial 91 finished with value: 3.9635323722549782 and parameters: {'learning_rate': 0.20063861650823894, 'lambda_l1': 0.31143967345933804, 'lambda_l2': 0.0011547525506089363, 'num_leaves': 27, 'feature_fraction': 0.6235082013545367, 'bagging_fraction': 0.8822122514799765, 'bagging_freq': 4, 'min_child_samples': 57}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.632363\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's l1: 0.501333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.582736\n",
      "[200]\tvalid_0's l1: 0.606935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:48,465]\u001b[0m Trial 92 finished with value: 3.9379874226669127 and parameters: {'learning_rate': 0.13302893130350374, 'lambda_l1': 0.014464771309608116, 'lambda_l2': 0.004509142577419634, 'num_leaves': 48, 'feature_fraction': 0.7216676529211178, 'bagging_fraction': 0.37839915175209105, 'bagging_freq': 4, 'min_child_samples': 46}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's l1: 0.504965\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.643718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:49,700]\u001b[0m Trial 93 finished with value: 4.0969498525046655 and parameters: {'learning_rate': 0.25790229235608053, 'lambda_l1': 0.14963064379396238, 'lambda_l2': 0.0105074000495569, 'num_leaves': 20, 'feature_fraction': 0.5926905092038631, 'bagging_fraction': 0.5712424833791202, 'bagging_freq': 4, 'min_child_samples': 42}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.64723\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's l1: 0.521662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.643624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:51,500]\u001b[0m Trial 94 finished with value: 3.9878323832994 and parameters: {'learning_rate': 0.17820019443198679, 'lambda_l1': 0.488773595600247, 'lambda_l2': 4.475769492564306e-05, 'num_leaves': 36, 'feature_fraction': 0.6743085939023811, 'bagging_fraction': 0.6226377402378912, 'bagging_freq': 5, 'min_child_samples': 47}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.659147\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 0.50998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.526051\n",
      "[200]\tvalid_0's l1: 0.526236\n",
      "[300]\tvalid_0's l1: 0.531802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:52,225]\u001b[0m Trial 95 finished with value: 3.856396686318155 and parameters: {'learning_rate': 0.14544506785957756, 'lambda_l1': 1.065120761605334, 'lambda_l2': 0.0005164059195772471, 'num_leaves': 2, 'feature_fraction': 0.5579965640639828, 'bagging_fraction': 0.9740980062782857, 'bagging_freq': 4, 'min_child_samples': 35}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's l1: 0.523232\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.536845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:54,939]\u001b[0m Trial 96 finished with value: 4.006190379902032 and parameters: {'learning_rate': 0.20202720475111907, 'lambda_l1': 0.25603169943784515, 'lambda_l2': 0.00017683854857045863, 'num_leaves': 54, 'feature_fraction': 0.9933958109906154, 'bagging_fraction': 0.2577124893966514, 'bagging_freq': 6, 'min_child_samples': 54}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.563755\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's l1: 0.512601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.570197\n",
      "[200]\tvalid_0's l1: 0.586204\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's l1: 0.49049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:56,219]\u001b[0m Trial 97 finished with value: 3.8111267889188847 and parameters: {'learning_rate': 0.18689938039674545, 'lambda_l1': 6.074710049286329, 'lambda_l2': 0.007097183482749767, 'num_leaves': 11, 'feature_fraction': 0.7376656821623871, 'bagging_fraction': 0.9088580076233698, 'bagging_freq': 4, 'min_child_samples': 58}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.571541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:57,640]\u001b[0m Trial 98 finished with value: 3.825670856397799 and parameters: {'learning_rate': 0.2405012557568551, 'lambda_l1': 0.036425223851824055, 'lambda_l2': 0.061024903672104426, 'num_leaves': 25, 'feature_fraction': 0.6603417800908087, 'bagging_fraction': 0.9524945488273142, 'bagging_freq': 5, 'min_child_samples': 38}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.582278\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's l1: 0.491399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10741\n",
      "[LightGBM] [Info] Number of data points in the train set: 33184, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.841699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's l1: 0.525485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-12 22:52:59,811]\u001b[0m Trial 99 finished with value: 3.8437449359346805 and parameters: {'learning_rate': 0.22768242251498733, 'lambda_l1': 2.835513940614262, 'lambda_l2': 0.021167889228001054, 'num_leaves': 45, 'feature_fraction': 0.6194272012094679, 'bagging_fraction': 0.9997432297381326, 'bagging_freq': 3, 'min_child_samples': 26}. Best is trial 62 with value: 3.6002945885329267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's l1: 0.529752\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's l1: 0.486007\n",
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1338962411650117,\n",
       " 'lambda_l1': 0.48074511048310864,\n",
       " 'lambda_l2': 0.0011040836215668386,\n",
       " 'num_leaves': 9,\n",
       " 'feature_fraction': 0.7102293153172105,\n",
       " 'bagging_fraction': 0.9176552911274246,\n",
       " 'bagging_freq': 4,\n",
       " 'min_child_samples': 37}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train[train['period_start_dt'] < '2019-12-01'].copy()\n",
    "test1 = train[(train['period_start_dt'] >= '2019-12-01') & (train['period_start_dt'] < '2019-12-31')].copy()\n",
    "\n",
    "X_train = train1[cols]\n",
    "y_train = train1['demand']\n",
    "    \n",
    "X_val = test1[cols]\n",
    "y_val = test1['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ed911\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10745\n",
      "[LightGBM] [Info] Number of data points in the train set: 34144, number of used features: 54\n",
      "[LightGBM] [Info] Start training from score 1.834218\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lgb_params = {'boosting_type': 'gbdt',\n",
    "              'metric': 'mae',\n",
    "              'n_jobs': -1,\n",
    "              'n_estimators': 5000,\n",
    "              'seed': 1,\n",
    "#               'early_stopping_rounds': 200,\n",
    "              'learning_rate': 0.23292247530870516,\n",
    "              'lambda_l1': 1.1693557095071368e-06,\n",
    "              'lambda_l2': 0.0056848325061162,\n",
    "              'num_leaves': 13,\n",
    "              'feature_fraction': 0.9424769602960624,\n",
    "              'bagging_fraction': 0.5936670517046974,\n",
    "              'bagging_freq': 6,\n",
    "              'min_child_samples': 16\n",
    "              }\n",
    "\n",
    "loss = []\n",
    "\n",
    "    \n",
    "lgbtrain = lgb.Dataset(data=X_train, label=y_train, feature_name=cols)\n",
    "#     lgbval = lgb.Dataset(data=X_val, label=y_val, reference=lgbtrain, feature_name=cols)\n",
    "    \n",
    "model = lgb.train(lgb_params, lgbtrain,\n",
    "#                   valid_sets = lgbval,\n",
    "                  verbose_eval=100)\n",
    "    \n",
    "y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1200.000000\n",
       "mean       17.457624\n",
       "std        23.045698\n",
       "min         0.498230\n",
       "25%         3.445080\n",
       "50%         8.328894\n",
       "75%        22.724739\n",
       "max       179.118344\n",
       "Name: demand, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1['demand'] = np.exp(y_pred_val)\n",
    "test1.sort_values('id', inplace=True)\n",
    "test1['demand'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission_df = test1.loc[:, ['id', 'demand']]\n",
    "submission_df['predicted'] = submission_df['demand'].copy()\n",
    "submission_df.drop('demand', axis=1, inplace=True)\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>908</td>\n",
       "      <td>6.442410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>909</td>\n",
       "      <td>9.960334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>910</td>\n",
       "      <td>5.360724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>911</td>\n",
       "      <td>4.951014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>912</td>\n",
       "      <td>3.687183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35339</th>\n",
       "      <td>35537</td>\n",
       "      <td>33.335397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35340</th>\n",
       "      <td>35538</td>\n",
       "      <td>34.411558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35341</th>\n",
       "      <td>35539</td>\n",
       "      <td>24.536677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35342</th>\n",
       "      <td>35540</td>\n",
       "      <td>3.521956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35343</th>\n",
       "      <td>35541</td>\n",
       "      <td>6.478302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  predicted\n",
       "908      908   6.442410\n",
       "909      909   9.960334\n",
       "910      910   5.360724\n",
       "911      911   4.951014\n",
       "912      912   3.687183\n",
       "...      ...        ...\n",
       "35339  35537  33.335397\n",
       "35340  35538  34.411558\n",
       "35341  35539  24.536677\n",
       "35342  35540   3.521956\n",
       "35343  35541   6.478302\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
